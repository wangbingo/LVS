{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangbingo/LVS/blob/master/zam-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBgSuLHBZ9Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSdxYQh_aQ7u",
        "colab_type": "code",
        "outputId": "3b6e7157-19ff-45fd-fb90-e8ea4b667ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElTH1Z91bVeO",
        "colab_type": "code",
        "outputId": "a09d439c-434a-4d3d-de6c-b0b4e20ac723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!git clone https://github.com/developer0hye/ZAM.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ZAM'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 463 (delta 29), reused 0 (delta 0), pack-reused 403\u001b[K\n",
            "Receiving objects: 100% (463/463), 2.60 MiB | 17.96 MiB/s, done.\n",
            "Resolving deltas: 100% (208/208), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE_NfBSpbhgN",
        "colab_type": "code",
        "outputId": "d77ef247-63ee-40d6-a40e-b56bb7a28eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  ZAM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSH3T687bozt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd ZAM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVlvCctxb6E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ZAM/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmTjG3QCcoMD",
        "colab_type": "code",
        "outputId": "de04e487-2f80-4e15-c7e6-7993e6db8a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!ls ZAM"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention_module.py  figures\t\tmobilenet_zam.py  resnet_zam.py\n",
            "conf\t\t     lr_finder.py\tREADME.md\t  test.py\n",
            "data\t\t     mobilenet_cbam.py\tresnet_cbam.py\t  train.py\n",
            "dataset.py\t     mobilenet.py\tresnet.py\t  utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbr-YyPWcvl-",
        "colab_type": "code",
        "outputId": "bbb3f41c-951e-4863-9d85-87749c382b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-10 14:13:14--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  54.1MB/s    in 3.0s    \n",
            "\n",
            "2019-09-10 14:13:18 (54.1 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Gcf9q2cHja",
        "colab_type": "code",
        "outputId": "8e758625-3b76-44b8-ef91-4fcd3d9a832d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!tar zxvf  cifar-100-python.tar.gz "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-100-python/\n",
            "cifar-100-python/file.txt~\n",
            "cifar-100-python/train\n",
            "cifar-100-python/test\n",
            "cifar-100-python/meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVBzqGuNdSyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp  cifar-100-python/*.*  ZAM/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7E_aZGLdfkI",
        "colab_type": "code",
        "outputId": "2dcc2b3b-6d7f-4588-ad73-d81505b92bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ZAM/train.py -net resnetzam18"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n",
            "169009152it [00:01, 90668669.09it/s]                   \n",
            "Files already downloaded and verified\n",
            "Training Epoch: 1 [128/50000]\tLoss: 4.6175\tLR: 0.000256\n",
            "Training Epoch: 1 [256/50000]\tLoss: 4.6271\tLR: 0.000512\n",
            "Training Epoch: 1 [384/50000]\tLoss: 4.6281\tLR: 0.000767\n",
            "Training Epoch: 1 [512/50000]\tLoss: 4.6495\tLR: 0.001023\n",
            "Training Epoch: 1 [640/50000]\tLoss: 4.6582\tLR: 0.001279\n",
            "Training Epoch: 1 [768/50000]\tLoss: 4.6316\tLR: 0.001535\n",
            "Training Epoch: 1 [896/50000]\tLoss: 4.6207\tLR: 0.001790\n",
            "Training Epoch: 1 [1024/50000]\tLoss: 4.5962\tLR: 0.002046\n",
            "Training Epoch: 1 [1152/50000]\tLoss: 4.6039\tLR: 0.002302\n",
            "Training Epoch: 1 [1280/50000]\tLoss: 4.6133\tLR: 0.002558\n",
            "Training Epoch: 1 [1408/50000]\tLoss: 4.5919\tLR: 0.002813\n",
            "Training Epoch: 1 [1536/50000]\tLoss: 4.6158\tLR: 0.003069\n",
            "Training Epoch: 1 [1664/50000]\tLoss: 4.5655\tLR: 0.003325\n",
            "Training Epoch: 1 [1792/50000]\tLoss: 4.5969\tLR: 0.003581\n",
            "Training Epoch: 1 [1920/50000]\tLoss: 4.6083\tLR: 0.003836\n",
            "Training Epoch: 1 [2048/50000]\tLoss: 4.5711\tLR: 0.004092\n",
            "Training Epoch: 1 [2176/50000]\tLoss: 4.5788\tLR: 0.004348\n",
            "Training Epoch: 1 [2304/50000]\tLoss: 4.5312\tLR: 0.004604\n",
            "Training Epoch: 1 [2432/50000]\tLoss: 4.5223\tLR: 0.004859\n",
            "Training Epoch: 1 [2560/50000]\tLoss: 4.5645\tLR: 0.005115\n",
            "Training Epoch: 1 [2688/50000]\tLoss: 4.5220\tLR: 0.005371\n",
            "Training Epoch: 1 [2816/50000]\tLoss: 4.5035\tLR: 0.005627\n",
            "Training Epoch: 1 [2944/50000]\tLoss: 4.5379\tLR: 0.005882\n",
            "Training Epoch: 1 [3072/50000]\tLoss: 4.5451\tLR: 0.006138\n",
            "Training Epoch: 1 [3200/50000]\tLoss: 4.5166\tLR: 0.006394\n",
            "Training Epoch: 1 [3328/50000]\tLoss: 4.4390\tLR: 0.006650\n",
            "Training Epoch: 1 [3456/50000]\tLoss: 4.5792\tLR: 0.006905\n",
            "Training Epoch: 1 [3584/50000]\tLoss: 4.4529\tLR: 0.007161\n",
            "Training Epoch: 1 [3712/50000]\tLoss: 4.4585\tLR: 0.007417\n",
            "Training Epoch: 1 [3840/50000]\tLoss: 4.4567\tLR: 0.007673\n",
            "Training Epoch: 1 [3968/50000]\tLoss: 4.4851\tLR: 0.007928\n",
            "Training Epoch: 1 [4096/50000]\tLoss: 4.5320\tLR: 0.008184\n",
            "Training Epoch: 1 [4224/50000]\tLoss: 4.5466\tLR: 0.008440\n",
            "Training Epoch: 1 [4352/50000]\tLoss: 4.4068\tLR: 0.008696\n",
            "Training Epoch: 1 [4480/50000]\tLoss: 4.4474\tLR: 0.008951\n",
            "Training Epoch: 1 [4608/50000]\tLoss: 4.4789\tLR: 0.009207\n",
            "Training Epoch: 1 [4736/50000]\tLoss: 4.3747\tLR: 0.009463\n",
            "Training Epoch: 1 [4864/50000]\tLoss: 4.3573\tLR: 0.009719\n",
            "Training Epoch: 1 [4992/50000]\tLoss: 4.5791\tLR: 0.009974\n",
            "Training Epoch: 1 [5120/50000]\tLoss: 4.4457\tLR: 0.010230\n",
            "Training Epoch: 1 [5248/50000]\tLoss: 4.4047\tLR: 0.010486\n",
            "Training Epoch: 1 [5376/50000]\tLoss: 4.4254\tLR: 0.010742\n",
            "Training Epoch: 1 [5504/50000]\tLoss: 4.5610\tLR: 0.010997\n",
            "Training Epoch: 1 [5632/50000]\tLoss: 4.3329\tLR: 0.011253\n",
            "Training Epoch: 1 [5760/50000]\tLoss: 4.4153\tLR: 0.011509\n",
            "Training Epoch: 1 [5888/50000]\tLoss: 4.3758\tLR: 0.011765\n",
            "Training Epoch: 1 [6016/50000]\tLoss: 4.3079\tLR: 0.012020\n",
            "Training Epoch: 1 [6144/50000]\tLoss: 4.4149\tLR: 0.012276\n",
            "Training Epoch: 1 [6272/50000]\tLoss: 4.3776\tLR: 0.012532\n",
            "Training Epoch: 1 [6400/50000]\tLoss: 4.3144\tLR: 0.012788\n",
            "Training Epoch: 1 [6528/50000]\tLoss: 4.3633\tLR: 0.013043\n",
            "Training Epoch: 1 [6656/50000]\tLoss: 4.2809\tLR: 0.013299\n",
            "Training Epoch: 1 [6784/50000]\tLoss: 4.2671\tLR: 0.013555\n",
            "Training Epoch: 1 [6912/50000]\tLoss: 4.3506\tLR: 0.013811\n",
            "Training Epoch: 1 [7040/50000]\tLoss: 4.3219\tLR: 0.014066\n",
            "Training Epoch: 1 [7168/50000]\tLoss: 4.3474\tLR: 0.014322\n",
            "Training Epoch: 1 [7296/50000]\tLoss: 4.3931\tLR: 0.014578\n",
            "Training Epoch: 1 [7424/50000]\tLoss: 4.2227\tLR: 0.014834\n",
            "Training Epoch: 1 [7552/50000]\tLoss: 4.3888\tLR: 0.015090\n",
            "Training Epoch: 1 [7680/50000]\tLoss: 4.2572\tLR: 0.015345\n",
            "Training Epoch: 1 [7808/50000]\tLoss: 4.2252\tLR: 0.015601\n",
            "Training Epoch: 1 [7936/50000]\tLoss: 4.2470\tLR: 0.015857\n",
            "Training Epoch: 1 [8064/50000]\tLoss: 4.3147\tLR: 0.016113\n",
            "Training Epoch: 1 [8192/50000]\tLoss: 4.2621\tLR: 0.016368\n",
            "Training Epoch: 1 [8320/50000]\tLoss: 4.1946\tLR: 0.016624\n",
            "Training Epoch: 1 [8448/50000]\tLoss: 4.1931\tLR: 0.016880\n",
            "Training Epoch: 1 [8576/50000]\tLoss: 4.2007\tLR: 0.017136\n",
            "Training Epoch: 1 [8704/50000]\tLoss: 4.2251\tLR: 0.017391\n",
            "Training Epoch: 1 [8832/50000]\tLoss: 4.2277\tLR: 0.017647\n",
            "Training Epoch: 1 [8960/50000]\tLoss: 4.3185\tLR: 0.017903\n",
            "Training Epoch: 1 [9088/50000]\tLoss: 4.4001\tLR: 0.018159\n",
            "Training Epoch: 1 [9216/50000]\tLoss: 4.0302\tLR: 0.018414\n",
            "Training Epoch: 1 [9344/50000]\tLoss: 4.2219\tLR: 0.018670\n",
            "Training Epoch: 1 [9472/50000]\tLoss: 4.1308\tLR: 0.018926\n",
            "Training Epoch: 1 [9600/50000]\tLoss: 4.0684\tLR: 0.019182\n",
            "Training Epoch: 1 [9728/50000]\tLoss: 4.1074\tLR: 0.019437\n",
            "Training Epoch: 1 [9856/50000]\tLoss: 4.0795\tLR: 0.019693\n",
            "Training Epoch: 1 [9984/50000]\tLoss: 4.1046\tLR: 0.019949\n",
            "Training Epoch: 1 [10112/50000]\tLoss: 4.1024\tLR: 0.020205\n",
            "Training Epoch: 1 [10240/50000]\tLoss: 3.9715\tLR: 0.020460\n",
            "Training Epoch: 1 [10368/50000]\tLoss: 4.2353\tLR: 0.020716\n",
            "Training Epoch: 1 [10496/50000]\tLoss: 4.2063\tLR: 0.020972\n",
            "Training Epoch: 1 [10624/50000]\tLoss: 4.1867\tLR: 0.021228\n",
            "Training Epoch: 1 [10752/50000]\tLoss: 4.1563\tLR: 0.021483\n",
            "Training Epoch: 1 [10880/50000]\tLoss: 3.9300\tLR: 0.021739\n",
            "Training Epoch: 1 [11008/50000]\tLoss: 4.1522\tLR: 0.021995\n",
            "Training Epoch: 1 [11136/50000]\tLoss: 4.2157\tLR: 0.022251\n",
            "Training Epoch: 1 [11264/50000]\tLoss: 4.1700\tLR: 0.022506\n",
            "Training Epoch: 1 [11392/50000]\tLoss: 4.1213\tLR: 0.022762\n",
            "Training Epoch: 1 [11520/50000]\tLoss: 4.0655\tLR: 0.023018\n",
            "Training Epoch: 1 [11648/50000]\tLoss: 4.1676\tLR: 0.023274\n",
            "Training Epoch: 1 [11776/50000]\tLoss: 4.2234\tLR: 0.023529\n",
            "Training Epoch: 1 [11904/50000]\tLoss: 4.1687\tLR: 0.023785\n",
            "Training Epoch: 1 [12032/50000]\tLoss: 4.1050\tLR: 0.024041\n",
            "Training Epoch: 1 [12160/50000]\tLoss: 4.0002\tLR: 0.024297\n",
            "Training Epoch: 1 [12288/50000]\tLoss: 3.9694\tLR: 0.024552\n",
            "Training Epoch: 1 [12416/50000]\tLoss: 4.0248\tLR: 0.024808\n",
            "Training Epoch: 1 [12544/50000]\tLoss: 4.1934\tLR: 0.025064\n",
            "Training Epoch: 1 [12672/50000]\tLoss: 4.1978\tLR: 0.025320\n",
            "Training Epoch: 1 [12800/50000]\tLoss: 3.9872\tLR: 0.025575\n",
            "Training Epoch: 1 [12928/50000]\tLoss: 4.1491\tLR: 0.025831\n",
            "Training Epoch: 1 [13056/50000]\tLoss: 4.0399\tLR: 0.026087\n",
            "Training Epoch: 1 [13184/50000]\tLoss: 4.0854\tLR: 0.026343\n",
            "Training Epoch: 1 [13312/50000]\tLoss: 4.2331\tLR: 0.026598\n",
            "Training Epoch: 1 [13440/50000]\tLoss: 4.1094\tLR: 0.026854\n",
            "Training Epoch: 1 [13568/50000]\tLoss: 3.8960\tLR: 0.027110\n",
            "Training Epoch: 1 [13696/50000]\tLoss: 3.9872\tLR: 0.027366\n",
            "Training Epoch: 1 [13824/50000]\tLoss: 4.0476\tLR: 0.027621\n",
            "Training Epoch: 1 [13952/50000]\tLoss: 4.2498\tLR: 0.027877\n",
            "Training Epoch: 1 [14080/50000]\tLoss: 4.1078\tLR: 0.028133\n",
            "Training Epoch: 1 [14208/50000]\tLoss: 4.2131\tLR: 0.028389\n",
            "Training Epoch: 1 [14336/50000]\tLoss: 4.0213\tLR: 0.028645\n",
            "Training Epoch: 1 [14464/50000]\tLoss: 4.0436\tLR: 0.028900\n",
            "Training Epoch: 1 [14592/50000]\tLoss: 4.0800\tLR: 0.029156\n",
            "Training Epoch: 1 [14720/50000]\tLoss: 3.9924\tLR: 0.029412\n",
            "Training Epoch: 1 [14848/50000]\tLoss: 4.1254\tLR: 0.029668\n",
            "Training Epoch: 1 [14976/50000]\tLoss: 3.9435\tLR: 0.029923\n",
            "Training Epoch: 1 [15104/50000]\tLoss: 4.0709\tLR: 0.030179\n",
            "Training Epoch: 1 [15232/50000]\tLoss: 3.9902\tLR: 0.030435\n",
            "Training Epoch: 1 [15360/50000]\tLoss: 4.0232\tLR: 0.030691\n",
            "Training Epoch: 1 [15488/50000]\tLoss: 4.0612\tLR: 0.030946\n",
            "Training Epoch: 1 [15616/50000]\tLoss: 4.0973\tLR: 0.031202\n",
            "Training Epoch: 1 [15744/50000]\tLoss: 4.0222\tLR: 0.031458\n",
            "Training Epoch: 1 [15872/50000]\tLoss: 4.0173\tLR: 0.031714\n",
            "Training Epoch: 1 [16000/50000]\tLoss: 3.9983\tLR: 0.031969\n",
            "Training Epoch: 1 [16128/50000]\tLoss: 4.0919\tLR: 0.032225\n",
            "Training Epoch: 1 [16256/50000]\tLoss: 4.0298\tLR: 0.032481\n",
            "Training Epoch: 1 [16384/50000]\tLoss: 4.0044\tLR: 0.032737\n",
            "Training Epoch: 1 [16512/50000]\tLoss: 4.0330\tLR: 0.032992\n",
            "Training Epoch: 1 [16640/50000]\tLoss: 3.9748\tLR: 0.033248\n",
            "Training Epoch: 1 [16768/50000]\tLoss: 3.9630\tLR: 0.033504\n",
            "Training Epoch: 1 [16896/50000]\tLoss: 3.8096\tLR: 0.033760\n",
            "Training Epoch: 1 [17024/50000]\tLoss: 3.8970\tLR: 0.034015\n",
            "Training Epoch: 1 [17152/50000]\tLoss: 3.9896\tLR: 0.034271\n",
            "Training Epoch: 1 [17280/50000]\tLoss: 3.9061\tLR: 0.034527\n",
            "Training Epoch: 1 [17408/50000]\tLoss: 4.0783\tLR: 0.034783\n",
            "Training Epoch: 1 [17536/50000]\tLoss: 3.7582\tLR: 0.035038\n",
            "Training Epoch: 1 [17664/50000]\tLoss: 4.1249\tLR: 0.035294\n",
            "Training Epoch: 1 [17792/50000]\tLoss: 4.1696\tLR: 0.035550\n",
            "Training Epoch: 1 [17920/50000]\tLoss: 3.7979\tLR: 0.035806\n",
            "Training Epoch: 1 [18048/50000]\tLoss: 3.9463\tLR: 0.036061\n",
            "Training Epoch: 1 [18176/50000]\tLoss: 3.9469\tLR: 0.036317\n",
            "Training Epoch: 1 [18304/50000]\tLoss: 4.0112\tLR: 0.036573\n",
            "Training Epoch: 1 [18432/50000]\tLoss: 3.9602\tLR: 0.036829\n",
            "Training Epoch: 1 [18560/50000]\tLoss: 3.9835\tLR: 0.037084\n",
            "Training Epoch: 1 [18688/50000]\tLoss: 4.1276\tLR: 0.037340\n",
            "Training Epoch: 1 [18816/50000]\tLoss: 3.9104\tLR: 0.037596\n",
            "Training Epoch: 1 [18944/50000]\tLoss: 3.8713\tLR: 0.037852\n",
            "Training Epoch: 1 [19072/50000]\tLoss: 4.0243\tLR: 0.038107\n",
            "Training Epoch: 1 [19200/50000]\tLoss: 3.9067\tLR: 0.038363\n",
            "Training Epoch: 1 [19328/50000]\tLoss: 3.9644\tLR: 0.038619\n",
            "Training Epoch: 1 [19456/50000]\tLoss: 3.7652\tLR: 0.038875\n",
            "Training Epoch: 1 [19584/50000]\tLoss: 3.9281\tLR: 0.039130\n",
            "Training Epoch: 1 [19712/50000]\tLoss: 4.0485\tLR: 0.039386\n",
            "Training Epoch: 1 [19840/50000]\tLoss: 3.9188\tLR: 0.039642\n",
            "Training Epoch: 1 [19968/50000]\tLoss: 4.1620\tLR: 0.039898\n",
            "Training Epoch: 1 [20096/50000]\tLoss: 3.8968\tLR: 0.040153\n",
            "Training Epoch: 1 [20224/50000]\tLoss: 3.8316\tLR: 0.040409\n",
            "Training Epoch: 1 [20352/50000]\tLoss: 3.8634\tLR: 0.040665\n",
            "Training Epoch: 1 [20480/50000]\tLoss: 3.8445\tLR: 0.040921\n",
            "Training Epoch: 1 [20608/50000]\tLoss: 3.9737\tLR: 0.041176\n",
            "Training Epoch: 1 [20736/50000]\tLoss: 3.9899\tLR: 0.041432\n",
            "Training Epoch: 1 [20864/50000]\tLoss: 4.0574\tLR: 0.041688\n",
            "Training Epoch: 1 [20992/50000]\tLoss: 3.9003\tLR: 0.041944\n",
            "Training Epoch: 1 [21120/50000]\tLoss: 4.0025\tLR: 0.042199\n",
            "Training Epoch: 1 [21248/50000]\tLoss: 3.9868\tLR: 0.042455\n",
            "Training Epoch: 1 [21376/50000]\tLoss: 3.8524\tLR: 0.042711\n",
            "Training Epoch: 1 [21504/50000]\tLoss: 3.9564\tLR: 0.042967\n",
            "Training Epoch: 1 [21632/50000]\tLoss: 3.8367\tLR: 0.043223\n",
            "Training Epoch: 1 [21760/50000]\tLoss: 4.0403\tLR: 0.043478\n",
            "Training Epoch: 1 [21888/50000]\tLoss: 3.9576\tLR: 0.043734\n",
            "Training Epoch: 1 [22016/50000]\tLoss: 3.9128\tLR: 0.043990\n",
            "Training Epoch: 1 [22144/50000]\tLoss: 3.9796\tLR: 0.044246\n",
            "Training Epoch: 1 [22272/50000]\tLoss: 4.1452\tLR: 0.044501\n",
            "Training Epoch: 1 [22400/50000]\tLoss: 3.9150\tLR: 0.044757\n",
            "Training Epoch: 1 [22528/50000]\tLoss: 3.9403\tLR: 0.045013\n",
            "Training Epoch: 1 [22656/50000]\tLoss: 3.8910\tLR: 0.045269\n",
            "Training Epoch: 1 [22784/50000]\tLoss: 3.7495\tLR: 0.045524\n",
            "Training Epoch: 1 [22912/50000]\tLoss: 3.7866\tLR: 0.045780\n",
            "Training Epoch: 1 [23040/50000]\tLoss: 3.9116\tLR: 0.046036\n",
            "Training Epoch: 1 [23168/50000]\tLoss: 4.1044\tLR: 0.046292\n",
            "Training Epoch: 1 [23296/50000]\tLoss: 3.9805\tLR: 0.046547\n",
            "Training Epoch: 1 [23424/50000]\tLoss: 4.0405\tLR: 0.046803\n",
            "Training Epoch: 1 [23552/50000]\tLoss: 4.0263\tLR: 0.047059\n",
            "Training Epoch: 1 [23680/50000]\tLoss: 3.9449\tLR: 0.047315\n",
            "Training Epoch: 1 [23808/50000]\tLoss: 3.9768\tLR: 0.047570\n",
            "Training Epoch: 1 [23936/50000]\tLoss: 3.8688\tLR: 0.047826\n",
            "Training Epoch: 1 [24064/50000]\tLoss: 4.0137\tLR: 0.048082\n",
            "Training Epoch: 1 [24192/50000]\tLoss: 4.0201\tLR: 0.048338\n",
            "Training Epoch: 1 [24320/50000]\tLoss: 4.0447\tLR: 0.048593\n",
            "Training Epoch: 1 [24448/50000]\tLoss: 3.9596\tLR: 0.048849\n",
            "Training Epoch: 1 [24576/50000]\tLoss: 3.8627\tLR: 0.049105\n",
            "Training Epoch: 1 [24704/50000]\tLoss: 3.7768\tLR: 0.049361\n",
            "Training Epoch: 1 [24832/50000]\tLoss: 3.9259\tLR: 0.049616\n",
            "Training Epoch: 1 [24960/50000]\tLoss: 3.7578\tLR: 0.049872\n",
            "Training Epoch: 1 [25088/50000]\tLoss: 3.9492\tLR: 0.050128\n",
            "Training Epoch: 1 [25216/50000]\tLoss: 3.8499\tLR: 0.050384\n",
            "Training Epoch: 1 [25344/50000]\tLoss: 3.8669\tLR: 0.050639\n",
            "Training Epoch: 1 [25472/50000]\tLoss: 3.9430\tLR: 0.050895\n",
            "Training Epoch: 1 [25600/50000]\tLoss: 3.7968\tLR: 0.051151\n",
            "Training Epoch: 1 [25728/50000]\tLoss: 3.7835\tLR: 0.051407\n",
            "Training Epoch: 1 [25856/50000]\tLoss: 3.7226\tLR: 0.051662\n",
            "Training Epoch: 1 [25984/50000]\tLoss: 3.7998\tLR: 0.051918\n",
            "Training Epoch: 1 [26112/50000]\tLoss: 3.6518\tLR: 0.052174\n",
            "Training Epoch: 1 [26240/50000]\tLoss: 3.6471\tLR: 0.052430\n",
            "Training Epoch: 1 [26368/50000]\tLoss: 3.7832\tLR: 0.052685\n",
            "Training Epoch: 1 [26496/50000]\tLoss: 3.7015\tLR: 0.052941\n",
            "Training Epoch: 1 [26624/50000]\tLoss: 3.8592\tLR: 0.053197\n",
            "Training Epoch: 1 [26752/50000]\tLoss: 3.8150\tLR: 0.053453\n",
            "Training Epoch: 1 [26880/50000]\tLoss: 3.8296\tLR: 0.053708\n",
            "Training Epoch: 1 [27008/50000]\tLoss: 3.8599\tLR: 0.053964\n",
            "Training Epoch: 1 [27136/50000]\tLoss: 3.8838\tLR: 0.054220\n",
            "Training Epoch: 1 [27264/50000]\tLoss: 3.4729\tLR: 0.054476\n",
            "Training Epoch: 1 [27392/50000]\tLoss: 3.7700\tLR: 0.054731\n",
            "Training Epoch: 1 [27520/50000]\tLoss: 3.9025\tLR: 0.054987\n",
            "Training Epoch: 1 [27648/50000]\tLoss: 3.7982\tLR: 0.055243\n",
            "Training Epoch: 1 [27776/50000]\tLoss: 3.8218\tLR: 0.055499\n",
            "Training Epoch: 1 [27904/50000]\tLoss: 3.6727\tLR: 0.055754\n",
            "Training Epoch: 1 [28032/50000]\tLoss: 3.7416\tLR: 0.056010\n",
            "Training Epoch: 1 [28160/50000]\tLoss: 3.8894\tLR: 0.056266\n",
            "Training Epoch: 1 [28288/50000]\tLoss: 3.7663\tLR: 0.056522\n",
            "Training Epoch: 1 [28416/50000]\tLoss: 3.6336\tLR: 0.056777\n",
            "Training Epoch: 1 [28544/50000]\tLoss: 3.5312\tLR: 0.057033\n",
            "Training Epoch: 1 [28672/50000]\tLoss: 3.8070\tLR: 0.057289\n",
            "Training Epoch: 1 [28800/50000]\tLoss: 3.8148\tLR: 0.057545\n",
            "Training Epoch: 1 [28928/50000]\tLoss: 3.8772\tLR: 0.057801\n",
            "Training Epoch: 1 [29056/50000]\tLoss: 3.8433\tLR: 0.058056\n",
            "Training Epoch: 1 [29184/50000]\tLoss: 3.7161\tLR: 0.058312\n",
            "Training Epoch: 1 [29312/50000]\tLoss: 3.7251\tLR: 0.058568\n",
            "Training Epoch: 1 [29440/50000]\tLoss: 3.7191\tLR: 0.058824\n",
            "Training Epoch: 1 [29568/50000]\tLoss: 3.8637\tLR: 0.059079\n",
            "Training Epoch: 1 [29696/50000]\tLoss: 3.7668\tLR: 0.059335\n",
            "Training Epoch: 1 [29824/50000]\tLoss: 3.6662\tLR: 0.059591\n",
            "Training Epoch: 1 [29952/50000]\tLoss: 3.9898\tLR: 0.059847\n",
            "Training Epoch: 1 [30080/50000]\tLoss: 3.9147\tLR: 0.060102\n",
            "Training Epoch: 1 [30208/50000]\tLoss: 3.6870\tLR: 0.060358\n",
            "Training Epoch: 1 [30336/50000]\tLoss: 3.8413\tLR: 0.060614\n",
            "Training Epoch: 1 [30464/50000]\tLoss: 3.8085\tLR: 0.060870\n",
            "Training Epoch: 1 [30592/50000]\tLoss: 4.0036\tLR: 0.061125\n",
            "Training Epoch: 1 [30720/50000]\tLoss: 3.9007\tLR: 0.061381\n",
            "Training Epoch: 1 [30848/50000]\tLoss: 3.8110\tLR: 0.061637\n",
            "Training Epoch: 1 [30976/50000]\tLoss: 3.8453\tLR: 0.061893\n",
            "Training Epoch: 1 [31104/50000]\tLoss: 3.8552\tLR: 0.062148\n",
            "Training Epoch: 1 [31232/50000]\tLoss: 3.9108\tLR: 0.062404\n",
            "Training Epoch: 1 [31360/50000]\tLoss: 3.8236\tLR: 0.062660\n",
            "Training Epoch: 1 [31488/50000]\tLoss: 3.8664\tLR: 0.062916\n",
            "Training Epoch: 1 [31616/50000]\tLoss: 3.6143\tLR: 0.063171\n",
            "Training Epoch: 1 [31744/50000]\tLoss: 3.7890\tLR: 0.063427\n",
            "Training Epoch: 1 [31872/50000]\tLoss: 3.7660\tLR: 0.063683\n",
            "Training Epoch: 1 [32000/50000]\tLoss: 3.5313\tLR: 0.063939\n",
            "Training Epoch: 1 [32128/50000]\tLoss: 3.9617\tLR: 0.064194\n",
            "Training Epoch: 1 [32256/50000]\tLoss: 3.6190\tLR: 0.064450\n",
            "Training Epoch: 1 [32384/50000]\tLoss: 3.6556\tLR: 0.064706\n",
            "Training Epoch: 1 [32512/50000]\tLoss: 3.4884\tLR: 0.064962\n",
            "Training Epoch: 1 [32640/50000]\tLoss: 3.6649\tLR: 0.065217\n",
            "Training Epoch: 1 [32768/50000]\tLoss: 3.8312\tLR: 0.065473\n",
            "Training Epoch: 1 [32896/50000]\tLoss: 3.5771\tLR: 0.065729\n",
            "Training Epoch: 1 [33024/50000]\tLoss: 3.8117\tLR: 0.065985\n",
            "Training Epoch: 1 [33152/50000]\tLoss: 3.8992\tLR: 0.066240\n",
            "Training Epoch: 1 [33280/50000]\tLoss: 4.0270\tLR: 0.066496\n",
            "Training Epoch: 1 [33408/50000]\tLoss: 3.8693\tLR: 0.066752\n",
            "Training Epoch: 1 [33536/50000]\tLoss: 3.6500\tLR: 0.067008\n",
            "Training Epoch: 1 [33664/50000]\tLoss: 3.7983\tLR: 0.067263\n",
            "Training Epoch: 1 [33792/50000]\tLoss: 3.6734\tLR: 0.067519\n",
            "Training Epoch: 1 [33920/50000]\tLoss: 3.6536\tLR: 0.067775\n",
            "Training Epoch: 1 [34048/50000]\tLoss: 3.7673\tLR: 0.068031\n",
            "Training Epoch: 1 [34176/50000]\tLoss: 3.5474\tLR: 0.068286\n",
            "Training Epoch: 1 [34304/50000]\tLoss: 3.9063\tLR: 0.068542\n",
            "Training Epoch: 1 [34432/50000]\tLoss: 3.7226\tLR: 0.068798\n",
            "Training Epoch: 1 [34560/50000]\tLoss: 3.9186\tLR: 0.069054\n",
            "Training Epoch: 1 [34688/50000]\tLoss: 3.5924\tLR: 0.069309\n",
            "Training Epoch: 1 [34816/50000]\tLoss: 3.8547\tLR: 0.069565\n",
            "Training Epoch: 1 [34944/50000]\tLoss: 3.6295\tLR: 0.069821\n",
            "Training Epoch: 1 [35072/50000]\tLoss: 3.7836\tLR: 0.070077\n",
            "Training Epoch: 1 [35200/50000]\tLoss: 3.6142\tLR: 0.070332\n",
            "Training Epoch: 1 [35328/50000]\tLoss: 4.0380\tLR: 0.070588\n",
            "Training Epoch: 1 [35456/50000]\tLoss: 3.8114\tLR: 0.070844\n",
            "Training Epoch: 1 [35584/50000]\tLoss: 3.8292\tLR: 0.071100\n",
            "Training Epoch: 1 [35712/50000]\tLoss: 3.7733\tLR: 0.071355\n",
            "Training Epoch: 1 [35840/50000]\tLoss: 3.8457\tLR: 0.071611\n",
            "Training Epoch: 1 [35968/50000]\tLoss: 3.5294\tLR: 0.071867\n",
            "Training Epoch: 1 [36096/50000]\tLoss: 3.7968\tLR: 0.072123\n",
            "Training Epoch: 1 [36224/50000]\tLoss: 3.7473\tLR: 0.072379\n",
            "Training Epoch: 1 [36352/50000]\tLoss: 3.7688\tLR: 0.072634\n",
            "Training Epoch: 1 [36480/50000]\tLoss: 3.6661\tLR: 0.072890\n",
            "Training Epoch: 1 [36608/50000]\tLoss: 3.7275\tLR: 0.073146\n",
            "Training Epoch: 1 [36736/50000]\tLoss: 3.6834\tLR: 0.073402\n",
            "Training Epoch: 1 [36864/50000]\tLoss: 4.0093\tLR: 0.073657\n",
            "Training Epoch: 1 [36992/50000]\tLoss: 3.8345\tLR: 0.073913\n",
            "Training Epoch: 1 [37120/50000]\tLoss: 3.8327\tLR: 0.074169\n",
            "Training Epoch: 1 [37248/50000]\tLoss: 3.8776\tLR: 0.074425\n",
            "Training Epoch: 1 [37376/50000]\tLoss: 3.7398\tLR: 0.074680\n",
            "Training Epoch: 1 [37504/50000]\tLoss: 3.7127\tLR: 0.074936\n",
            "Training Epoch: 1 [37632/50000]\tLoss: 3.6766\tLR: 0.075192\n",
            "Training Epoch: 1 [37760/50000]\tLoss: 3.6108\tLR: 0.075448\n",
            "Training Epoch: 1 [37888/50000]\tLoss: 3.6720\tLR: 0.075703\n",
            "Training Epoch: 1 [38016/50000]\tLoss: 3.8242\tLR: 0.075959\n",
            "Training Epoch: 1 [38144/50000]\tLoss: 3.6743\tLR: 0.076215\n",
            "Training Epoch: 1 [38272/50000]\tLoss: 3.6372\tLR: 0.076471\n",
            "Training Epoch: 1 [38400/50000]\tLoss: 3.8367\tLR: 0.076726\n",
            "Training Epoch: 1 [38528/50000]\tLoss: 3.6750\tLR: 0.076982\n",
            "Training Epoch: 1 [38656/50000]\tLoss: 3.7700\tLR: 0.077238\n",
            "Training Epoch: 1 [38784/50000]\tLoss: 3.7727\tLR: 0.077494\n",
            "Training Epoch: 1 [38912/50000]\tLoss: 3.7595\tLR: 0.077749\n",
            "Training Epoch: 1 [39040/50000]\tLoss: 3.6619\tLR: 0.078005\n",
            "Training Epoch: 1 [39168/50000]\tLoss: 3.5353\tLR: 0.078261\n",
            "Training Epoch: 1 [39296/50000]\tLoss: 3.6399\tLR: 0.078517\n",
            "Training Epoch: 1 [39424/50000]\tLoss: 3.7585\tLR: 0.078772\n",
            "Training Epoch: 1 [39552/50000]\tLoss: 3.7645\tLR: 0.079028\n",
            "Training Epoch: 1 [39680/50000]\tLoss: 3.6627\tLR: 0.079284\n",
            "Training Epoch: 1 [39808/50000]\tLoss: 3.8553\tLR: 0.079540\n",
            "Training Epoch: 1 [39936/50000]\tLoss: 3.7754\tLR: 0.079795\n",
            "Training Epoch: 1 [40064/50000]\tLoss: 3.7176\tLR: 0.080051\n",
            "Training Epoch: 1 [40192/50000]\tLoss: 3.7101\tLR: 0.080307\n",
            "Training Epoch: 1 [40320/50000]\tLoss: 3.6865\tLR: 0.080563\n",
            "Training Epoch: 1 [40448/50000]\tLoss: 3.6494\tLR: 0.080818\n",
            "Training Epoch: 1 [40576/50000]\tLoss: 3.6107\tLR: 0.081074\n",
            "Training Epoch: 1 [40704/50000]\tLoss: 3.7536\tLR: 0.081330\n",
            "Training Epoch: 1 [40832/50000]\tLoss: 3.6186\tLR: 0.081586\n",
            "Training Epoch: 1 [40960/50000]\tLoss: 3.5782\tLR: 0.081841\n",
            "Training Epoch: 1 [41088/50000]\tLoss: 3.6122\tLR: 0.082097\n",
            "Training Epoch: 1 [41216/50000]\tLoss: 3.7522\tLR: 0.082353\n",
            "Training Epoch: 1 [41344/50000]\tLoss: 3.5794\tLR: 0.082609\n",
            "Training Epoch: 1 [41472/50000]\tLoss: 3.6169\tLR: 0.082864\n",
            "Training Epoch: 1 [41600/50000]\tLoss: 3.5260\tLR: 0.083120\n",
            "Training Epoch: 1 [41728/50000]\tLoss: 3.6662\tLR: 0.083376\n",
            "Training Epoch: 1 [41856/50000]\tLoss: 3.5951\tLR: 0.083632\n",
            "Training Epoch: 1 [41984/50000]\tLoss: 3.5659\tLR: 0.083887\n",
            "Training Epoch: 1 [42112/50000]\tLoss: 3.5824\tLR: 0.084143\n",
            "Training Epoch: 1 [42240/50000]\tLoss: 3.6511\tLR: 0.084399\n",
            "Training Epoch: 1 [42368/50000]\tLoss: 3.7833\tLR: 0.084655\n",
            "Training Epoch: 1 [42496/50000]\tLoss: 3.3530\tLR: 0.084910\n",
            "Training Epoch: 1 [42624/50000]\tLoss: 3.3689\tLR: 0.085166\n",
            "Training Epoch: 1 [42752/50000]\tLoss: 3.6356\tLR: 0.085422\n",
            "Training Epoch: 1 [42880/50000]\tLoss: 3.6500\tLR: 0.085678\n",
            "Training Epoch: 1 [43008/50000]\tLoss: 3.3320\tLR: 0.085934\n",
            "Training Epoch: 1 [43136/50000]\tLoss: 3.5395\tLR: 0.086189\n",
            "Training Epoch: 1 [43264/50000]\tLoss: 3.4500\tLR: 0.086445\n",
            "Training Epoch: 1 [43392/50000]\tLoss: 3.9261\tLR: 0.086701\n",
            "Training Epoch: 1 [43520/50000]\tLoss: 3.7485\tLR: 0.086957\n",
            "Training Epoch: 1 [43648/50000]\tLoss: 3.6254\tLR: 0.087212\n",
            "Training Epoch: 1 [43776/50000]\tLoss: 3.3593\tLR: 0.087468\n",
            "Training Epoch: 1 [43904/50000]\tLoss: 3.7995\tLR: 0.087724\n",
            "Training Epoch: 1 [44032/50000]\tLoss: 3.5952\tLR: 0.087980\n",
            "Training Epoch: 1 [44160/50000]\tLoss: 3.5634\tLR: 0.088235\n",
            "Training Epoch: 1 [44288/50000]\tLoss: 3.7694\tLR: 0.088491\n",
            "Training Epoch: 1 [44416/50000]\tLoss: 3.5778\tLR: 0.088747\n",
            "Training Epoch: 1 [44544/50000]\tLoss: 3.7400\tLR: 0.089003\n",
            "Training Epoch: 1 [44672/50000]\tLoss: 3.6585\tLR: 0.089258\n",
            "Training Epoch: 1 [44800/50000]\tLoss: 3.5112\tLR: 0.089514\n",
            "Training Epoch: 1 [44928/50000]\tLoss: 3.6657\tLR: 0.089770\n",
            "Training Epoch: 1 [45056/50000]\tLoss: 3.6467\tLR: 0.090026\n",
            "Training Epoch: 1 [45184/50000]\tLoss: 3.8494\tLR: 0.090281\n",
            "Training Epoch: 1 [45312/50000]\tLoss: 3.9820\tLR: 0.090537\n",
            "Training Epoch: 1 [45440/50000]\tLoss: 3.6171\tLR: 0.090793\n",
            "Training Epoch: 1 [45568/50000]\tLoss: 3.5876\tLR: 0.091049\n",
            "Training Epoch: 1 [45696/50000]\tLoss: 3.5183\tLR: 0.091304\n",
            "Training Epoch: 1 [45824/50000]\tLoss: 3.5335\tLR: 0.091560\n",
            "Training Epoch: 1 [45952/50000]\tLoss: 3.8420\tLR: 0.091816\n",
            "Training Epoch: 1 [46080/50000]\tLoss: 3.6506\tLR: 0.092072\n",
            "Training Epoch: 1 [46208/50000]\tLoss: 3.6838\tLR: 0.092327\n",
            "Training Epoch: 1 [46336/50000]\tLoss: 3.6055\tLR: 0.092583\n",
            "Training Epoch: 1 [46464/50000]\tLoss: 3.5104\tLR: 0.092839\n",
            "Training Epoch: 1 [46592/50000]\tLoss: 3.5582\tLR: 0.093095\n",
            "Training Epoch: 1 [46720/50000]\tLoss: 3.6008\tLR: 0.093350\n",
            "Training Epoch: 1 [46848/50000]\tLoss: 3.5974\tLR: 0.093606\n",
            "Training Epoch: 1 [46976/50000]\tLoss: 3.6542\tLR: 0.093862\n",
            "Training Epoch: 1 [47104/50000]\tLoss: 3.6067\tLR: 0.094118\n",
            "Training Epoch: 1 [47232/50000]\tLoss: 3.5762\tLR: 0.094373\n",
            "Training Epoch: 1 [47360/50000]\tLoss: 3.5449\tLR: 0.094629\n",
            "Training Epoch: 1 [47488/50000]\tLoss: 3.5655\tLR: 0.094885\n",
            "Training Epoch: 1 [47616/50000]\tLoss: 3.6931\tLR: 0.095141\n",
            "Training Epoch: 1 [47744/50000]\tLoss: 3.5189\tLR: 0.095396\n",
            "Training Epoch: 1 [47872/50000]\tLoss: 3.3735\tLR: 0.095652\n",
            "Training Epoch: 1 [48000/50000]\tLoss: 3.7598\tLR: 0.095908\n",
            "Training Epoch: 1 [48128/50000]\tLoss: 3.3344\tLR: 0.096164\n",
            "Training Epoch: 1 [48256/50000]\tLoss: 3.6555\tLR: 0.096419\n",
            "Training Epoch: 1 [48384/50000]\tLoss: 3.5094\tLR: 0.096675\n",
            "Training Epoch: 1 [48512/50000]\tLoss: 3.5239\tLR: 0.096931\n",
            "Training Epoch: 1 [48640/50000]\tLoss: 3.6878\tLR: 0.097187\n",
            "Training Epoch: 1 [48768/50000]\tLoss: 3.7052\tLR: 0.097442\n",
            "Training Epoch: 1 [48896/50000]\tLoss: 3.8082\tLR: 0.097698\n",
            "Training Epoch: 1 [49024/50000]\tLoss: 3.5519\tLR: 0.097954\n",
            "Training Epoch: 1 [49152/50000]\tLoss: 3.3908\tLR: 0.098210\n",
            "Training Epoch: 1 [49280/50000]\tLoss: 3.6387\tLR: 0.098465\n",
            "Training Epoch: 1 [49408/50000]\tLoss: 3.7544\tLR: 0.098721\n",
            "Training Epoch: 1 [49536/50000]\tLoss: 3.6612\tLR: 0.098977\n",
            "Training Epoch: 1 [49664/50000]\tLoss: 3.4824\tLR: 0.099233\n",
            "Training Epoch: 1 [49792/50000]\tLoss: 3.5656\tLR: 0.099488\n",
            "Training Epoch: 1 [49920/50000]\tLoss: 3.4698\tLR: 0.099744\n",
            "Training Epoch: 1 [50000/50000]\tLoss: 3.5365\tLR: 0.100000\n",
            "Test set: Average loss: 0.0302, Accuracy: 0.1195\n",
            "\n",
            "Training Epoch: 2 [128/50000]\tLoss: 3.4588\tLR: 0.100000\n",
            "Training Epoch: 2 [256/50000]\tLoss: 3.4222\tLR: 0.100000\n",
            "Training Epoch: 2 [384/50000]\tLoss: 3.6480\tLR: 0.100000\n",
            "Training Epoch: 2 [512/50000]\tLoss: 3.6171\tLR: 0.100000\n",
            "Training Epoch: 2 [640/50000]\tLoss: 3.5749\tLR: 0.100000\n",
            "Training Epoch: 2 [768/50000]\tLoss: 3.5554\tLR: 0.100000\n",
            "Training Epoch: 2 [896/50000]\tLoss: 3.6402\tLR: 0.100000\n",
            "Training Epoch: 2 [1024/50000]\tLoss: 3.5720\tLR: 0.100000\n",
            "Training Epoch: 2 [1152/50000]\tLoss: 3.5502\tLR: 0.100000\n",
            "Training Epoch: 2 [1280/50000]\tLoss: 3.5466\tLR: 0.100000\n",
            "Training Epoch: 2 [1408/50000]\tLoss: 3.3786\tLR: 0.100000\n",
            "Training Epoch: 2 [1536/50000]\tLoss: 3.4531\tLR: 0.100000\n",
            "Training Epoch: 2 [1664/50000]\tLoss: 3.3899\tLR: 0.100000\n",
            "Training Epoch: 2 [1792/50000]\tLoss: 3.5517\tLR: 0.100000\n",
            "Training Epoch: 2 [1920/50000]\tLoss: 3.5495\tLR: 0.100000\n",
            "Training Epoch: 2 [2048/50000]\tLoss: 3.7474\tLR: 0.100000\n",
            "Training Epoch: 2 [2176/50000]\tLoss: 3.5050\tLR: 0.100000\n",
            "Training Epoch: 2 [2304/50000]\tLoss: 3.6420\tLR: 0.100000\n",
            "Training Epoch: 2 [2432/50000]\tLoss: 3.5514\tLR: 0.100000\n",
            "Training Epoch: 2 [2560/50000]\tLoss: 3.4019\tLR: 0.100000\n",
            "Training Epoch: 2 [2688/50000]\tLoss: 3.4382\tLR: 0.100000\n",
            "Training Epoch: 2 [2816/50000]\tLoss: 3.4599\tLR: 0.100000\n",
            "Training Epoch: 2 [2944/50000]\tLoss: 3.2953\tLR: 0.100000\n",
            "Training Epoch: 2 [3072/50000]\tLoss: 3.3479\tLR: 0.100000\n",
            "Training Epoch: 2 [3200/50000]\tLoss: 3.4222\tLR: 0.100000\n",
            "Training Epoch: 2 [3328/50000]\tLoss: 3.4815\tLR: 0.100000\n",
            "Training Epoch: 2 [3456/50000]\tLoss: 3.4863\tLR: 0.100000\n",
            "Training Epoch: 2 [3584/50000]\tLoss: 3.4843\tLR: 0.100000\n",
            "Training Epoch: 2 [3712/50000]\tLoss: 3.4982\tLR: 0.100000\n",
            "Training Epoch: 2 [3840/50000]\tLoss: 3.5513\tLR: 0.100000\n",
            "Training Epoch: 2 [3968/50000]\tLoss: 3.4245\tLR: 0.100000\n",
            "Training Epoch: 2 [4096/50000]\tLoss: 3.4750\tLR: 0.100000\n",
            "Training Epoch: 2 [4224/50000]\tLoss: 3.4851\tLR: 0.100000\n",
            "Training Epoch: 2 [4352/50000]\tLoss: 3.3798\tLR: 0.100000\n",
            "Training Epoch: 2 [4480/50000]\tLoss: 3.6019\tLR: 0.100000\n",
            "Training Epoch: 2 [4608/50000]\tLoss: 3.4684\tLR: 0.100000\n",
            "Training Epoch: 2 [4736/50000]\tLoss: 3.3532\tLR: 0.100000\n",
            "Training Epoch: 2 [4864/50000]\tLoss: 3.5821\tLR: 0.100000\n",
            "Training Epoch: 2 [4992/50000]\tLoss: 3.3498\tLR: 0.100000\n",
            "Training Epoch: 2 [5120/50000]\tLoss: 3.4892\tLR: 0.100000\n",
            "Training Epoch: 2 [5248/50000]\tLoss: 3.4392\tLR: 0.100000\n",
            "Training Epoch: 2 [5376/50000]\tLoss: 3.3354\tLR: 0.100000\n",
            "Training Epoch: 2 [5504/50000]\tLoss: 3.3576\tLR: 0.100000\n",
            "Training Epoch: 2 [5632/50000]\tLoss: 3.3651\tLR: 0.100000\n",
            "Training Epoch: 2 [5760/50000]\tLoss: 3.6154\tLR: 0.100000\n",
            "Training Epoch: 2 [5888/50000]\tLoss: 3.4944\tLR: 0.100000\n",
            "Training Epoch: 2 [6016/50000]\tLoss: 3.5893\tLR: 0.100000\n",
            "Training Epoch: 2 [6144/50000]\tLoss: 3.4349\tLR: 0.100000\n",
            "Training Epoch: 2 [6272/50000]\tLoss: 3.3079\tLR: 0.100000\n",
            "Training Epoch: 2 [6400/50000]\tLoss: 3.6676\tLR: 0.100000\n",
            "Training Epoch: 2 [6528/50000]\tLoss: 3.5367\tLR: 0.100000\n",
            "Training Epoch: 2 [6656/50000]\tLoss: 3.5537\tLR: 0.100000\n",
            "Training Epoch: 2 [6784/50000]\tLoss: 3.3648\tLR: 0.100000\n",
            "Training Epoch: 2 [6912/50000]\tLoss: 3.3125\tLR: 0.100000\n",
            "Training Epoch: 2 [7040/50000]\tLoss: 3.2265\tLR: 0.100000\n",
            "Training Epoch: 2 [7168/50000]\tLoss: 3.3989\tLR: 0.100000\n",
            "Training Epoch: 2 [7296/50000]\tLoss: 3.4095\tLR: 0.100000\n",
            "Training Epoch: 2 [7424/50000]\tLoss: 3.4000\tLR: 0.100000\n",
            "Training Epoch: 2 [7552/50000]\tLoss: 3.2333\tLR: 0.100000\n",
            "Training Epoch: 2 [7680/50000]\tLoss: 3.4790\tLR: 0.100000\n",
            "Training Epoch: 2 [7808/50000]\tLoss: 3.4828\tLR: 0.100000\n",
            "Training Epoch: 2 [7936/50000]\tLoss: 3.4483\tLR: 0.100000\n",
            "Training Epoch: 2 [8064/50000]\tLoss: 3.5300\tLR: 0.100000\n",
            "Training Epoch: 2 [8192/50000]\tLoss: 3.4823\tLR: 0.100000\n",
            "Training Epoch: 2 [8320/50000]\tLoss: 3.3260\tLR: 0.100000\n",
            "Training Epoch: 2 [8448/50000]\tLoss: 3.5932\tLR: 0.100000\n",
            "Training Epoch: 2 [8576/50000]\tLoss: 3.4410\tLR: 0.100000\n",
            "Training Epoch: 2 [8704/50000]\tLoss: 3.4606\tLR: 0.100000\n",
            "Training Epoch: 2 [8832/50000]\tLoss: 3.3425\tLR: 0.100000\n",
            "Training Epoch: 2 [8960/50000]\tLoss: 3.4869\tLR: 0.100000\n",
            "Training Epoch: 2 [9088/50000]\tLoss: 3.3026\tLR: 0.100000\n",
            "Training Epoch: 2 [9216/50000]\tLoss: 3.2504\tLR: 0.100000\n",
            "Training Epoch: 2 [9344/50000]\tLoss: 3.2980\tLR: 0.100000\n",
            "Training Epoch: 2 [9472/50000]\tLoss: 3.4894\tLR: 0.100000\n",
            "Training Epoch: 2 [9600/50000]\tLoss: 3.3017\tLR: 0.100000\n",
            "Training Epoch: 2 [9728/50000]\tLoss: 3.3056\tLR: 0.100000\n",
            "Training Epoch: 2 [9856/50000]\tLoss: 3.3223\tLR: 0.100000\n",
            "Training Epoch: 2 [9984/50000]\tLoss: 3.2202\tLR: 0.100000\n",
            "Training Epoch: 2 [10112/50000]\tLoss: 3.1632\tLR: 0.100000\n",
            "Training Epoch: 2 [10240/50000]\tLoss: 3.2825\tLR: 0.100000\n",
            "Training Epoch: 2 [10368/50000]\tLoss: 3.3841\tLR: 0.100000\n",
            "Training Epoch: 2 [10496/50000]\tLoss: 3.4235\tLR: 0.100000\n",
            "Training Epoch: 2 [10624/50000]\tLoss: 3.4463\tLR: 0.100000\n",
            "Training Epoch: 2 [10752/50000]\tLoss: 3.2166\tLR: 0.100000\n",
            "Training Epoch: 2 [10880/50000]\tLoss: 3.2663\tLR: 0.100000\n",
            "Training Epoch: 2 [11008/50000]\tLoss: 3.3634\tLR: 0.100000\n",
            "Training Epoch: 2 [11136/50000]\tLoss: 3.3202\tLR: 0.100000\n",
            "Training Epoch: 2 [11264/50000]\tLoss: 3.2621\tLR: 0.100000\n",
            "Training Epoch: 2 [11392/50000]\tLoss: 3.3275\tLR: 0.100000\n",
            "Training Epoch: 2 [11520/50000]\tLoss: 3.4498\tLR: 0.100000\n",
            "Training Epoch: 2 [11648/50000]\tLoss: 3.5255\tLR: 0.100000\n",
            "Training Epoch: 2 [11776/50000]\tLoss: 3.3803\tLR: 0.100000\n",
            "Training Epoch: 2 [11904/50000]\tLoss: 3.2714\tLR: 0.100000\n",
            "Training Epoch: 2 [12032/50000]\tLoss: 3.3507\tLR: 0.100000\n",
            "Training Epoch: 2 [12160/50000]\tLoss: 3.2798\tLR: 0.100000\n",
            "Training Epoch: 2 [12288/50000]\tLoss: 3.5351\tLR: 0.100000\n",
            "Training Epoch: 2 [12416/50000]\tLoss: 3.3916\tLR: 0.100000\n",
            "Training Epoch: 2 [12544/50000]\tLoss: 3.2466\tLR: 0.100000\n",
            "Training Epoch: 2 [12672/50000]\tLoss: 3.1157\tLR: 0.100000\n",
            "Training Epoch: 2 [12800/50000]\tLoss: 3.2470\tLR: 0.100000\n",
            "Training Epoch: 2 [12928/50000]\tLoss: 3.6297\tLR: 0.100000\n",
            "Training Epoch: 2 [13056/50000]\tLoss: 3.3898\tLR: 0.100000\n",
            "Training Epoch: 2 [13184/50000]\tLoss: 3.3114\tLR: 0.100000\n",
            "Training Epoch: 2 [13312/50000]\tLoss: 3.3968\tLR: 0.100000\n",
            "Training Epoch: 2 [13440/50000]\tLoss: 3.2466\tLR: 0.100000\n",
            "Training Epoch: 2 [13568/50000]\tLoss: 3.3620\tLR: 0.100000\n",
            "Training Epoch: 2 [13696/50000]\tLoss: 3.3381\tLR: 0.100000\n",
            "Training Epoch: 2 [13824/50000]\tLoss: 3.2121\tLR: 0.100000\n",
            "Training Epoch: 2 [13952/50000]\tLoss: 3.0983\tLR: 0.100000\n",
            "Training Epoch: 2 [14080/50000]\tLoss: 3.2909\tLR: 0.100000\n",
            "Training Epoch: 2 [14208/50000]\tLoss: 3.2188\tLR: 0.100000\n",
            "Training Epoch: 2 [14336/50000]\tLoss: 3.0388\tLR: 0.100000\n",
            "Training Epoch: 2 [14464/50000]\tLoss: 3.3486\tLR: 0.100000\n",
            "Training Epoch: 2 [14592/50000]\tLoss: 3.4805\tLR: 0.100000\n",
            "Training Epoch: 2 [14720/50000]\tLoss: 3.2320\tLR: 0.100000\n",
            "Training Epoch: 2 [14848/50000]\tLoss: 3.3547\tLR: 0.100000\n",
            "Training Epoch: 2 [14976/50000]\tLoss: 3.2673\tLR: 0.100000\n",
            "Training Epoch: 2 [15104/50000]\tLoss: 3.4551\tLR: 0.100000\n",
            "Training Epoch: 2 [15232/50000]\tLoss: 3.1316\tLR: 0.100000\n",
            "Training Epoch: 2 [15360/50000]\tLoss: 3.4926\tLR: 0.100000\n",
            "Training Epoch: 2 [15488/50000]\tLoss: 3.4418\tLR: 0.100000\n",
            "Training Epoch: 2 [15616/50000]\tLoss: 3.3594\tLR: 0.100000\n",
            "Training Epoch: 2 [15744/50000]\tLoss: 3.6109\tLR: 0.100000\n",
            "Training Epoch: 2 [15872/50000]\tLoss: 3.1999\tLR: 0.100000\n",
            "Training Epoch: 2 [16000/50000]\tLoss: 3.3398\tLR: 0.100000\n",
            "Training Epoch: 2 [16128/50000]\tLoss: 3.3018\tLR: 0.100000\n",
            "Training Epoch: 2 [16256/50000]\tLoss: 3.2884\tLR: 0.100000\n",
            "Training Epoch: 2 [16384/50000]\tLoss: 3.5062\tLR: 0.100000\n",
            "Training Epoch: 2 [16512/50000]\tLoss: 3.3233\tLR: 0.100000\n",
            "Training Epoch: 2 [16640/50000]\tLoss: 3.1172\tLR: 0.100000\n",
            "Training Epoch: 2 [16768/50000]\tLoss: 3.4191\tLR: 0.100000\n",
            "Training Epoch: 2 [16896/50000]\tLoss: 3.4203\tLR: 0.100000\n",
            "Training Epoch: 2 [17024/50000]\tLoss: 3.1130\tLR: 0.100000\n",
            "Training Epoch: 2 [17152/50000]\tLoss: 3.2589\tLR: 0.100000\n",
            "Training Epoch: 2 [17280/50000]\tLoss: 3.4300\tLR: 0.100000\n",
            "Training Epoch: 2 [17408/50000]\tLoss: 3.3660\tLR: 0.100000\n",
            "Training Epoch: 2 [17536/50000]\tLoss: 3.3772\tLR: 0.100000\n",
            "Training Epoch: 2 [17664/50000]\tLoss: 3.0664\tLR: 0.100000\n",
            "Training Epoch: 2 [17792/50000]\tLoss: 3.2850\tLR: 0.100000\n",
            "Training Epoch: 2 [17920/50000]\tLoss: 3.3028\tLR: 0.100000\n",
            "Training Epoch: 2 [18048/50000]\tLoss: 3.3014\tLR: 0.100000\n",
            "Training Epoch: 2 [18176/50000]\tLoss: 3.2701\tLR: 0.100000\n",
            "Training Epoch: 2 [18304/50000]\tLoss: 3.5016\tLR: 0.100000\n",
            "Training Epoch: 2 [18432/50000]\tLoss: 3.1961\tLR: 0.100000\n",
            "Training Epoch: 2 [18560/50000]\tLoss: 3.2436\tLR: 0.100000\n",
            "Training Epoch: 2 [18688/50000]\tLoss: 3.0928\tLR: 0.100000\n",
            "Training Epoch: 2 [18816/50000]\tLoss: 3.3149\tLR: 0.100000\n",
            "Training Epoch: 2 [18944/50000]\tLoss: 2.9355\tLR: 0.100000\n",
            "Training Epoch: 2 [19072/50000]\tLoss: 3.2247\tLR: 0.100000\n",
            "Training Epoch: 2 [19200/50000]\tLoss: 3.1000\tLR: 0.100000\n",
            "Training Epoch: 2 [19328/50000]\tLoss: 3.4804\tLR: 0.100000\n",
            "Training Epoch: 2 [19456/50000]\tLoss: 3.3771\tLR: 0.100000\n",
            "Training Epoch: 2 [19584/50000]\tLoss: 3.2330\tLR: 0.100000\n",
            "Training Epoch: 2 [19712/50000]\tLoss: 3.2131\tLR: 0.100000\n",
            "Training Epoch: 2 [19840/50000]\tLoss: 3.4449\tLR: 0.100000\n",
            "Training Epoch: 2 [19968/50000]\tLoss: 3.1039\tLR: 0.100000\n",
            "Training Epoch: 2 [20096/50000]\tLoss: 3.0714\tLR: 0.100000\n",
            "Training Epoch: 2 [20224/50000]\tLoss: 3.4406\tLR: 0.100000\n",
            "Training Epoch: 2 [20352/50000]\tLoss: 3.1391\tLR: 0.100000\n",
            "Training Epoch: 2 [20480/50000]\tLoss: 3.0599\tLR: 0.100000\n",
            "Training Epoch: 2 [20608/50000]\tLoss: 3.2501\tLR: 0.100000\n",
            "Training Epoch: 2 [20736/50000]\tLoss: 3.2574\tLR: 0.100000\n",
            "Training Epoch: 2 [20864/50000]\tLoss: 3.4145\tLR: 0.100000\n",
            "Training Epoch: 2 [20992/50000]\tLoss: 3.2003\tLR: 0.100000\n",
            "Training Epoch: 2 [21120/50000]\tLoss: 3.2590\tLR: 0.100000\n",
            "Training Epoch: 2 [21248/50000]\tLoss: 3.2723\tLR: 0.100000\n",
            "Training Epoch: 2 [21376/50000]\tLoss: 2.9049\tLR: 0.100000\n",
            "Training Epoch: 2 [21504/50000]\tLoss: 3.1807\tLR: 0.100000\n",
            "Training Epoch: 2 [21632/50000]\tLoss: 3.2601\tLR: 0.100000\n",
            "Training Epoch: 2 [21760/50000]\tLoss: 3.5332\tLR: 0.100000\n",
            "Training Epoch: 2 [21888/50000]\tLoss: 3.1070\tLR: 0.100000\n",
            "Training Epoch: 2 [22016/50000]\tLoss: 3.1286\tLR: 0.100000\n",
            "Training Epoch: 2 [22144/50000]\tLoss: 3.2686\tLR: 0.100000\n",
            "Training Epoch: 2 [22272/50000]\tLoss: 3.1559\tLR: 0.100000\n",
            "Training Epoch: 2 [22400/50000]\tLoss: 3.3677\tLR: 0.100000\n",
            "Training Epoch: 2 [22528/50000]\tLoss: 2.9054\tLR: 0.100000\n",
            "Training Epoch: 2 [22656/50000]\tLoss: 3.1962\tLR: 0.100000\n",
            "Training Epoch: 2 [22784/50000]\tLoss: 3.2381\tLR: 0.100000\n",
            "Training Epoch: 2 [22912/50000]\tLoss: 3.1069\tLR: 0.100000\n",
            "Training Epoch: 2 [23040/50000]\tLoss: 3.1100\tLR: 0.100000\n",
            "Training Epoch: 2 [23168/50000]\tLoss: 2.9650\tLR: 0.100000\n",
            "Training Epoch: 2 [23296/50000]\tLoss: 3.1944\tLR: 0.100000\n",
            "Training Epoch: 2 [23424/50000]\tLoss: 3.2921\tLR: 0.100000\n",
            "Training Epoch: 2 [23552/50000]\tLoss: 3.2212\tLR: 0.100000\n",
            "Training Epoch: 2 [23680/50000]\tLoss: 3.1947\tLR: 0.100000\n",
            "Training Epoch: 2 [23808/50000]\tLoss: 3.0504\tLR: 0.100000\n",
            "Training Epoch: 2 [23936/50000]\tLoss: 3.4614\tLR: 0.100000\n",
            "Training Epoch: 2 [24064/50000]\tLoss: 3.0609\tLR: 0.100000\n",
            "Training Epoch: 2 [24192/50000]\tLoss: 3.1224\tLR: 0.100000\n",
            "Training Epoch: 2 [24320/50000]\tLoss: 3.0511\tLR: 0.100000\n",
            "Training Epoch: 2 [24448/50000]\tLoss: 3.2887\tLR: 0.100000\n",
            "Training Epoch: 2 [24576/50000]\tLoss: 3.0610\tLR: 0.100000\n",
            "Training Epoch: 2 [24704/50000]\tLoss: 3.3110\tLR: 0.100000\n",
            "Training Epoch: 2 [24832/50000]\tLoss: 3.2986\tLR: 0.100000\n",
            "Training Epoch: 2 [24960/50000]\tLoss: 3.1064\tLR: 0.100000\n",
            "Training Epoch: 2 [25088/50000]\tLoss: 3.4371\tLR: 0.100000\n",
            "Training Epoch: 2 [25216/50000]\tLoss: 3.3074\tLR: 0.100000\n",
            "Training Epoch: 2 [25344/50000]\tLoss: 3.2624\tLR: 0.100000\n",
            "Training Epoch: 2 [25472/50000]\tLoss: 3.2801\tLR: 0.100000\n",
            "Training Epoch: 2 [25600/50000]\tLoss: 3.2298\tLR: 0.100000\n",
            "Training Epoch: 2 [25728/50000]\tLoss: 3.0894\tLR: 0.100000\n",
            "Training Epoch: 2 [25856/50000]\tLoss: 3.3034\tLR: 0.100000\n",
            "Training Epoch: 2 [25984/50000]\tLoss: 3.1998\tLR: 0.100000\n",
            "Training Epoch: 2 [26112/50000]\tLoss: 3.1752\tLR: 0.100000\n",
            "Training Epoch: 2 [26240/50000]\tLoss: 3.1107\tLR: 0.100000\n",
            "Training Epoch: 2 [26368/50000]\tLoss: 3.3013\tLR: 0.100000\n",
            "Training Epoch: 2 [26496/50000]\tLoss: 3.3810\tLR: 0.100000\n",
            "Training Epoch: 2 [26624/50000]\tLoss: 3.2993\tLR: 0.100000\n",
            "Training Epoch: 2 [26752/50000]\tLoss: 3.2431\tLR: 0.100000\n",
            "Training Epoch: 2 [26880/50000]\tLoss: 3.3496\tLR: 0.100000\n",
            "Training Epoch: 2 [27008/50000]\tLoss: 3.0276\tLR: 0.100000\n",
            "Training Epoch: 2 [27136/50000]\tLoss: 3.0799\tLR: 0.100000\n",
            "Training Epoch: 2 [27264/50000]\tLoss: 3.0043\tLR: 0.100000\n",
            "Training Epoch: 2 [27392/50000]\tLoss: 3.2519\tLR: 0.100000\n",
            "Training Epoch: 2 [27520/50000]\tLoss: 3.2229\tLR: 0.100000\n",
            "Training Epoch: 2 [27648/50000]\tLoss: 3.4473\tLR: 0.100000\n",
            "Training Epoch: 2 [27776/50000]\tLoss: 3.2070\tLR: 0.100000\n",
            "Training Epoch: 2 [27904/50000]\tLoss: 3.1520\tLR: 0.100000\n",
            "Training Epoch: 2 [28032/50000]\tLoss: 3.4661\tLR: 0.100000\n",
            "Training Epoch: 2 [28160/50000]\tLoss: 3.2927\tLR: 0.100000\n",
            "Training Epoch: 2 [28288/50000]\tLoss: 3.2485\tLR: 0.100000\n",
            "Training Epoch: 2 [28416/50000]\tLoss: 3.2197\tLR: 0.100000\n",
            "Training Epoch: 2 [28544/50000]\tLoss: 3.1207\tLR: 0.100000\n",
            "Training Epoch: 2 [28672/50000]\tLoss: 3.2149\tLR: 0.100000\n",
            "Training Epoch: 2 [28800/50000]\tLoss: 3.1537\tLR: 0.100000\n",
            "Training Epoch: 2 [28928/50000]\tLoss: 3.1989\tLR: 0.100000\n",
            "Training Epoch: 2 [29056/50000]\tLoss: 3.2059\tLR: 0.100000\n",
            "Training Epoch: 2 [29184/50000]\tLoss: 3.2451\tLR: 0.100000\n",
            "Training Epoch: 2 [29312/50000]\tLoss: 3.1459\tLR: 0.100000\n",
            "Training Epoch: 2 [29440/50000]\tLoss: 3.1427\tLR: 0.100000\n",
            "Training Epoch: 2 [29568/50000]\tLoss: 3.2880\tLR: 0.100000\n",
            "Training Epoch: 2 [29696/50000]\tLoss: 3.2495\tLR: 0.100000\n",
            "Training Epoch: 2 [29824/50000]\tLoss: 3.2349\tLR: 0.100000\n",
            "Training Epoch: 2 [29952/50000]\tLoss: 2.8987\tLR: 0.100000\n",
            "Training Epoch: 2 [30080/50000]\tLoss: 3.0740\tLR: 0.100000\n",
            "Training Epoch: 2 [30208/50000]\tLoss: 3.3545\tLR: 0.100000\n",
            "Training Epoch: 2 [30336/50000]\tLoss: 3.1793\tLR: 0.100000\n",
            "Training Epoch: 2 [30464/50000]\tLoss: 3.3123\tLR: 0.100000\n",
            "Training Epoch: 2 [30592/50000]\tLoss: 3.1505\tLR: 0.100000\n",
            "Training Epoch: 2 [30720/50000]\tLoss: 3.1993\tLR: 0.100000\n",
            "Training Epoch: 2 [30848/50000]\tLoss: 3.1851\tLR: 0.100000\n",
            "Training Epoch: 2 [30976/50000]\tLoss: 2.9827\tLR: 0.100000\n",
            "Training Epoch: 2 [31104/50000]\tLoss: 2.9562\tLR: 0.100000\n",
            "Training Epoch: 2 [31232/50000]\tLoss: 3.0159\tLR: 0.100000\n",
            "Training Epoch: 2 [31360/50000]\tLoss: 3.3005\tLR: 0.100000\n",
            "Training Epoch: 2 [31488/50000]\tLoss: 3.0802\tLR: 0.100000\n",
            "Training Epoch: 2 [31616/50000]\tLoss: 3.2325\tLR: 0.100000\n",
            "Training Epoch: 2 [31744/50000]\tLoss: 3.1389\tLR: 0.100000\n",
            "Training Epoch: 2 [31872/50000]\tLoss: 3.2202\tLR: 0.100000\n",
            "Training Epoch: 2 [32000/50000]\tLoss: 3.0933\tLR: 0.100000\n",
            "Training Epoch: 2 [32128/50000]\tLoss: 3.2410\tLR: 0.100000\n",
            "Training Epoch: 2 [32256/50000]\tLoss: 3.1892\tLR: 0.100000\n",
            "Training Epoch: 2 [32384/50000]\tLoss: 2.9793\tLR: 0.100000\n",
            "Training Epoch: 2 [32512/50000]\tLoss: 3.1346\tLR: 0.100000\n",
            "Training Epoch: 2 [32640/50000]\tLoss: 3.2185\tLR: 0.100000\n",
            "Training Epoch: 2 [32768/50000]\tLoss: 3.0956\tLR: 0.100000\n",
            "Training Epoch: 2 [32896/50000]\tLoss: 3.0688\tLR: 0.100000\n",
            "Training Epoch: 2 [33024/50000]\tLoss: 3.1679\tLR: 0.100000\n",
            "Training Epoch: 2 [33152/50000]\tLoss: 3.3020\tLR: 0.100000\n",
            "Training Epoch: 2 [33280/50000]\tLoss: 3.1279\tLR: 0.100000\n",
            "Training Epoch: 2 [33408/50000]\tLoss: 3.0028\tLR: 0.100000\n",
            "Training Epoch: 2 [33536/50000]\tLoss: 3.2876\tLR: 0.100000\n",
            "Training Epoch: 2 [33664/50000]\tLoss: 3.1218\tLR: 0.100000\n",
            "Training Epoch: 2 [33792/50000]\tLoss: 3.1240\tLR: 0.100000\n",
            "Training Epoch: 2 [33920/50000]\tLoss: 3.2026\tLR: 0.100000\n",
            "Training Epoch: 2 [34048/50000]\tLoss: 3.0814\tLR: 0.100000\n",
            "Training Epoch: 2 [34176/50000]\tLoss: 3.0334\tLR: 0.100000\n",
            "Training Epoch: 2 [34304/50000]\tLoss: 3.3371\tLR: 0.100000\n",
            "Training Epoch: 2 [34432/50000]\tLoss: 3.1215\tLR: 0.100000\n",
            "Training Epoch: 2 [34560/50000]\tLoss: 3.1816\tLR: 0.100000\n",
            "Training Epoch: 2 [34688/50000]\tLoss: 3.2765\tLR: 0.100000\n",
            "Training Epoch: 2 [34816/50000]\tLoss: 3.3153\tLR: 0.100000\n",
            "Training Epoch: 2 [34944/50000]\tLoss: 3.2179\tLR: 0.100000\n",
            "Training Epoch: 2 [35072/50000]\tLoss: 2.8957\tLR: 0.100000\n",
            "Training Epoch: 2 [35200/50000]\tLoss: 3.2376\tLR: 0.100000\n",
            "Training Epoch: 2 [35328/50000]\tLoss: 3.2627\tLR: 0.100000\n",
            "Training Epoch: 2 [35456/50000]\tLoss: 3.2438\tLR: 0.100000\n",
            "Training Epoch: 2 [35584/50000]\tLoss: 3.0383\tLR: 0.100000\n",
            "Training Epoch: 2 [35712/50000]\tLoss: 3.1666\tLR: 0.100000\n",
            "Training Epoch: 2 [35840/50000]\tLoss: 3.1614\tLR: 0.100000\n",
            "Training Epoch: 2 [35968/50000]\tLoss: 3.2835\tLR: 0.100000\n",
            "Training Epoch: 2 [36096/50000]\tLoss: 3.1097\tLR: 0.100000\n",
            "Training Epoch: 2 [36224/50000]\tLoss: 3.0881\tLR: 0.100000\n",
            "Training Epoch: 2 [36352/50000]\tLoss: 2.9308\tLR: 0.100000\n",
            "Training Epoch: 2 [36480/50000]\tLoss: 3.0492\tLR: 0.100000\n",
            "Training Epoch: 2 [36608/50000]\tLoss: 2.9893\tLR: 0.100000\n",
            "Training Epoch: 2 [36736/50000]\tLoss: 3.0595\tLR: 0.100000\n",
            "Training Epoch: 2 [36864/50000]\tLoss: 3.1849\tLR: 0.100000\n",
            "Training Epoch: 2 [36992/50000]\tLoss: 3.1947\tLR: 0.100000\n",
            "Training Epoch: 2 [37120/50000]\tLoss: 2.9738\tLR: 0.100000\n",
            "Training Epoch: 2 [37248/50000]\tLoss: 3.0006\tLR: 0.100000\n",
            "Training Epoch: 2 [37376/50000]\tLoss: 3.0211\tLR: 0.100000\n",
            "Training Epoch: 2 [37504/50000]\tLoss: 3.1236\tLR: 0.100000\n",
            "Training Epoch: 2 [37632/50000]\tLoss: 2.9591\tLR: 0.100000\n",
            "Training Epoch: 2 [37760/50000]\tLoss: 3.1529\tLR: 0.100000\n",
            "Training Epoch: 2 [37888/50000]\tLoss: 3.0483\tLR: 0.100000\n",
            "Training Epoch: 2 [38016/50000]\tLoss: 3.0668\tLR: 0.100000\n",
            "Training Epoch: 2 [38144/50000]\tLoss: 3.0109\tLR: 0.100000\n",
            "Training Epoch: 2 [38272/50000]\tLoss: 3.1485\tLR: 0.100000\n",
            "Training Epoch: 2 [38400/50000]\tLoss: 3.0226\tLR: 0.100000\n",
            "Training Epoch: 2 [38528/50000]\tLoss: 3.2710\tLR: 0.100000\n",
            "Training Epoch: 2 [38656/50000]\tLoss: 3.1356\tLR: 0.100000\n",
            "Training Epoch: 2 [38784/50000]\tLoss: 3.3066\tLR: 0.100000\n",
            "Training Epoch: 2 [38912/50000]\tLoss: 3.0587\tLR: 0.100000\n",
            "Training Epoch: 2 [39040/50000]\tLoss: 3.2724\tLR: 0.100000\n",
            "Training Epoch: 2 [39168/50000]\tLoss: 3.0040\tLR: 0.100000\n",
            "Training Epoch: 2 [39296/50000]\tLoss: 3.3099\tLR: 0.100000\n",
            "Training Epoch: 2 [39424/50000]\tLoss: 3.0222\tLR: 0.100000\n",
            "Training Epoch: 2 [39552/50000]\tLoss: 3.1526\tLR: 0.100000\n",
            "Training Epoch: 2 [39680/50000]\tLoss: 3.1904\tLR: 0.100000\n",
            "Training Epoch: 2 [39808/50000]\tLoss: 2.8162\tLR: 0.100000\n",
            "Training Epoch: 2 [39936/50000]\tLoss: 3.1630\tLR: 0.100000\n",
            "Training Epoch: 2 [40064/50000]\tLoss: 3.0568\tLR: 0.100000\n",
            "Training Epoch: 2 [40192/50000]\tLoss: 2.9683\tLR: 0.100000\n",
            "Training Epoch: 2 [40320/50000]\tLoss: 2.9533\tLR: 0.100000\n",
            "Training Epoch: 2 [40448/50000]\tLoss: 2.9349\tLR: 0.100000\n",
            "Training Epoch: 2 [40576/50000]\tLoss: 3.0693\tLR: 0.100000\n",
            "Training Epoch: 2 [40704/50000]\tLoss: 2.8943\tLR: 0.100000\n",
            "Training Epoch: 2 [40832/50000]\tLoss: 3.0787\tLR: 0.100000\n",
            "Training Epoch: 2 [40960/50000]\tLoss: 3.1301\tLR: 0.100000\n",
            "Training Epoch: 2 [41088/50000]\tLoss: 2.9577\tLR: 0.100000\n",
            "Training Epoch: 2 [41216/50000]\tLoss: 3.1983\tLR: 0.100000\n",
            "Training Epoch: 2 [41344/50000]\tLoss: 2.9393\tLR: 0.100000\n",
            "Training Epoch: 2 [41472/50000]\tLoss: 3.0176\tLR: 0.100000\n",
            "Training Epoch: 2 [41600/50000]\tLoss: 3.0357\tLR: 0.100000\n",
            "Training Epoch: 2 [41728/50000]\tLoss: 3.0219\tLR: 0.100000\n",
            "Training Epoch: 2 [41856/50000]\tLoss: 2.9770\tLR: 0.100000\n",
            "Training Epoch: 2 [41984/50000]\tLoss: 2.6678\tLR: 0.100000\n",
            "Training Epoch: 2 [42112/50000]\tLoss: 2.8084\tLR: 0.100000\n",
            "Training Epoch: 2 [42240/50000]\tLoss: 3.0789\tLR: 0.100000\n",
            "Training Epoch: 2 [42368/50000]\tLoss: 2.9398\tLR: 0.100000\n",
            "Training Epoch: 2 [42496/50000]\tLoss: 3.1675\tLR: 0.100000\n",
            "Training Epoch: 2 [42624/50000]\tLoss: 2.9070\tLR: 0.100000\n",
            "Training Epoch: 2 [42752/50000]\tLoss: 3.2213\tLR: 0.100000\n",
            "Training Epoch: 2 [42880/50000]\tLoss: 2.9843\tLR: 0.100000\n",
            "Training Epoch: 2 [43008/50000]\tLoss: 2.9888\tLR: 0.100000\n",
            "Training Epoch: 2 [43136/50000]\tLoss: 3.3008\tLR: 0.100000\n",
            "Training Epoch: 2 [43264/50000]\tLoss: 3.2635\tLR: 0.100000\n",
            "Training Epoch: 2 [43392/50000]\tLoss: 2.8805\tLR: 0.100000\n",
            "Training Epoch: 2 [43520/50000]\tLoss: 3.2333\tLR: 0.100000\n",
            "Training Epoch: 2 [43648/50000]\tLoss: 3.1575\tLR: 0.100000\n",
            "Training Epoch: 2 [43776/50000]\tLoss: 3.0906\tLR: 0.100000\n",
            "Training Epoch: 2 [43904/50000]\tLoss: 3.1370\tLR: 0.100000\n",
            "Training Epoch: 2 [44032/50000]\tLoss: 3.0809\tLR: 0.100000\n",
            "Training Epoch: 2 [44160/50000]\tLoss: 3.0406\tLR: 0.100000\n",
            "Training Epoch: 2 [44288/50000]\tLoss: 3.1353\tLR: 0.100000\n",
            "Training Epoch: 2 [44416/50000]\tLoss: 2.9374\tLR: 0.100000\n",
            "Training Epoch: 2 [44544/50000]\tLoss: 2.9640\tLR: 0.100000\n",
            "Training Epoch: 2 [44672/50000]\tLoss: 3.2725\tLR: 0.100000\n",
            "Training Epoch: 2 [44800/50000]\tLoss: 2.7305\tLR: 0.100000\n",
            "Training Epoch: 2 [44928/50000]\tLoss: 3.1288\tLR: 0.100000\n",
            "Training Epoch: 2 [45056/50000]\tLoss: 3.1443\tLR: 0.100000\n",
            "Training Epoch: 2 [45184/50000]\tLoss: 3.0387\tLR: 0.100000\n",
            "Training Epoch: 2 [45312/50000]\tLoss: 3.0491\tLR: 0.100000\n",
            "Training Epoch: 2 [45440/50000]\tLoss: 3.1528\tLR: 0.100000\n",
            "Training Epoch: 2 [45568/50000]\tLoss: 2.8754\tLR: 0.100000\n",
            "Training Epoch: 2 [45696/50000]\tLoss: 2.8821\tLR: 0.100000\n",
            "Training Epoch: 2 [45824/50000]\tLoss: 2.8195\tLR: 0.100000\n",
            "Training Epoch: 2 [45952/50000]\tLoss: 2.8811\tLR: 0.100000\n",
            "Training Epoch: 2 [46080/50000]\tLoss: 3.2477\tLR: 0.100000\n",
            "Training Epoch: 2 [46208/50000]\tLoss: 3.3494\tLR: 0.100000\n",
            "Training Epoch: 2 [46336/50000]\tLoss: 2.8713\tLR: 0.100000\n",
            "Training Epoch: 2 [46464/50000]\tLoss: 3.0364\tLR: 0.100000\n",
            "Training Epoch: 2 [46592/50000]\tLoss: 2.9107\tLR: 0.100000\n",
            "Training Epoch: 2 [46720/50000]\tLoss: 3.1506\tLR: 0.100000\n",
            "Training Epoch: 2 [46848/50000]\tLoss: 2.9749\tLR: 0.100000\n",
            "Training Epoch: 2 [46976/50000]\tLoss: 3.0333\tLR: 0.100000\n",
            "Training Epoch: 2 [47104/50000]\tLoss: 3.1438\tLR: 0.100000\n",
            "Training Epoch: 2 [47232/50000]\tLoss: 2.9749\tLR: 0.100000\n",
            "Training Epoch: 2 [47360/50000]\tLoss: 3.0284\tLR: 0.100000\n",
            "Training Epoch: 2 [47488/50000]\tLoss: 2.9051\tLR: 0.100000\n",
            "Training Epoch: 2 [47616/50000]\tLoss: 2.9463\tLR: 0.100000\n",
            "Training Epoch: 2 [47744/50000]\tLoss: 2.8231\tLR: 0.100000\n",
            "Training Epoch: 2 [47872/50000]\tLoss: 2.9519\tLR: 0.100000\n",
            "Training Epoch: 2 [48000/50000]\tLoss: 2.9664\tLR: 0.100000\n",
            "Training Epoch: 2 [48128/50000]\tLoss: 2.8533\tLR: 0.100000\n",
            "Training Epoch: 2 [48256/50000]\tLoss: 3.1043\tLR: 0.100000\n",
            "Training Epoch: 2 [48384/50000]\tLoss: 3.0929\tLR: 0.100000\n",
            "Training Epoch: 2 [48512/50000]\tLoss: 3.0566\tLR: 0.100000\n",
            "Training Epoch: 2 [48640/50000]\tLoss: 3.0597\tLR: 0.100000\n",
            "Training Epoch: 2 [48768/50000]\tLoss: 2.9371\tLR: 0.100000\n",
            "Training Epoch: 2 [48896/50000]\tLoss: 2.9445\tLR: 0.100000\n",
            "Training Epoch: 2 [49024/50000]\tLoss: 3.1930\tLR: 0.100000\n",
            "Training Epoch: 2 [49152/50000]\tLoss: 3.0850\tLR: 0.100000\n",
            "Training Epoch: 2 [49280/50000]\tLoss: 2.8263\tLR: 0.100000\n",
            "Training Epoch: 2 [49408/50000]\tLoss: 3.0219\tLR: 0.100000\n",
            "Training Epoch: 2 [49536/50000]\tLoss: 3.1996\tLR: 0.100000\n",
            "Training Epoch: 2 [49664/50000]\tLoss: 2.8693\tLR: 0.100000\n",
            "Training Epoch: 2 [49792/50000]\tLoss: 3.0306\tLR: 0.100000\n",
            "Training Epoch: 2 [49920/50000]\tLoss: 3.2444\tLR: 0.100000\n",
            "Training Epoch: 2 [50000/50000]\tLoss: 2.9429\tLR: 0.100000\n",
            "Test set: Average loss: 0.0240, Accuracy: 0.2452\n",
            "\n",
            "Training Epoch: 3 [128/50000]\tLoss: 2.8002\tLR: 0.100000\n",
            "Training Epoch: 3 [256/50000]\tLoss: 3.0560\tLR: 0.100000\n",
            "Training Epoch: 3 [384/50000]\tLoss: 3.1189\tLR: 0.100000\n",
            "Training Epoch: 3 [512/50000]\tLoss: 3.0274\tLR: 0.100000\n",
            "Training Epoch: 3 [640/50000]\tLoss: 2.9217\tLR: 0.100000\n",
            "Training Epoch: 3 [768/50000]\tLoss: 2.9853\tLR: 0.100000\n",
            "Training Epoch: 3 [896/50000]\tLoss: 2.9726\tLR: 0.100000\n",
            "Training Epoch: 3 [1024/50000]\tLoss: 3.0590\tLR: 0.100000\n",
            "Training Epoch: 3 [1152/50000]\tLoss: 3.1537\tLR: 0.100000\n",
            "Training Epoch: 3 [1280/50000]\tLoss: 3.0348\tLR: 0.100000\n",
            "Training Epoch: 3 [1408/50000]\tLoss: 2.9021\tLR: 0.100000\n",
            "Training Epoch: 3 [1536/50000]\tLoss: 2.9814\tLR: 0.100000\n",
            "Training Epoch: 3 [1664/50000]\tLoss: 2.9201\tLR: 0.100000\n",
            "Training Epoch: 3 [1792/50000]\tLoss: 2.9291\tLR: 0.100000\n",
            "Training Epoch: 3 [1920/50000]\tLoss: 3.0087\tLR: 0.100000\n",
            "Training Epoch: 3 [2048/50000]\tLoss: 2.6782\tLR: 0.100000\n",
            "Training Epoch: 3 [2176/50000]\tLoss: 2.7985\tLR: 0.100000\n",
            "Training Epoch: 3 [2304/50000]\tLoss: 3.0411\tLR: 0.100000\n",
            "Training Epoch: 3 [2432/50000]\tLoss: 2.7399\tLR: 0.100000\n",
            "Training Epoch: 3 [2560/50000]\tLoss: 3.0183\tLR: 0.100000\n",
            "Training Epoch: 3 [2688/50000]\tLoss: 2.8951\tLR: 0.100000\n",
            "Training Epoch: 3 [2816/50000]\tLoss: 3.0442\tLR: 0.100000\n",
            "Training Epoch: 3 [2944/50000]\tLoss: 2.9705\tLR: 0.100000\n",
            "Training Epoch: 3 [3072/50000]\tLoss: 2.6633\tLR: 0.100000\n",
            "Training Epoch: 3 [3200/50000]\tLoss: 3.0210\tLR: 0.100000\n",
            "Training Epoch: 3 [3328/50000]\tLoss: 2.7731\tLR: 0.100000\n",
            "Training Epoch: 3 [3456/50000]\tLoss: 2.7402\tLR: 0.100000\n",
            "Training Epoch: 3 [3584/50000]\tLoss: 2.7988\tLR: 0.100000\n",
            "Training Epoch: 3 [3712/50000]\tLoss: 2.7825\tLR: 0.100000\n",
            "Training Epoch: 3 [3840/50000]\tLoss: 3.0217\tLR: 0.100000\n",
            "Training Epoch: 3 [3968/50000]\tLoss: 3.0554\tLR: 0.100000\n",
            "Training Epoch: 3 [4096/50000]\tLoss: 2.9336\tLR: 0.100000\n",
            "Training Epoch: 3 [4224/50000]\tLoss: 2.9033\tLR: 0.100000\n",
            "Training Epoch: 3 [4352/50000]\tLoss: 3.1191\tLR: 0.100000\n",
            "Training Epoch: 3 [4480/50000]\tLoss: 2.7704\tLR: 0.100000\n",
            "Training Epoch: 3 [4608/50000]\tLoss: 3.0595\tLR: 0.100000\n",
            "Training Epoch: 3 [4736/50000]\tLoss: 2.8156\tLR: 0.100000\n",
            "Training Epoch: 3 [4864/50000]\tLoss: 3.0690\tLR: 0.100000\n",
            "Training Epoch: 3 [4992/50000]\tLoss: 3.0183\tLR: 0.100000\n",
            "Training Epoch: 3 [5120/50000]\tLoss: 2.9004\tLR: 0.100000\n",
            "Training Epoch: 3 [5248/50000]\tLoss: 2.9844\tLR: 0.100000\n",
            "Training Epoch: 3 [5376/50000]\tLoss: 3.2033\tLR: 0.100000\n",
            "Training Epoch: 3 [5504/50000]\tLoss: 2.8681\tLR: 0.100000\n",
            "Training Epoch: 3 [5632/50000]\tLoss: 3.0733\tLR: 0.100000\n",
            "Training Epoch: 3 [5760/50000]\tLoss: 2.7672\tLR: 0.100000\n",
            "Training Epoch: 3 [5888/50000]\tLoss: 2.9691\tLR: 0.100000\n",
            "Training Epoch: 3 [6016/50000]\tLoss: 2.8496\tLR: 0.100000\n",
            "Training Epoch: 3 [6144/50000]\tLoss: 2.9381\tLR: 0.100000\n",
            "Training Epoch: 3 [6272/50000]\tLoss: 2.8223\tLR: 0.100000\n",
            "Training Epoch: 3 [6400/50000]\tLoss: 3.0200\tLR: 0.100000\n",
            "Training Epoch: 3 [6528/50000]\tLoss: 2.9715\tLR: 0.100000\n",
            "Training Epoch: 3 [6656/50000]\tLoss: 2.8228\tLR: 0.100000\n",
            "Training Epoch: 3 [6784/50000]\tLoss: 2.8122\tLR: 0.100000\n",
            "Training Epoch: 3 [6912/50000]\tLoss: 2.8476\tLR: 0.100000\n",
            "Training Epoch: 3 [7040/50000]\tLoss: 2.8241\tLR: 0.100000\n",
            "Training Epoch: 3 [7168/50000]\tLoss: 2.8475\tLR: 0.100000\n",
            "Training Epoch: 3 [7296/50000]\tLoss: 2.9022\tLR: 0.100000\n",
            "Training Epoch: 3 [7424/50000]\tLoss: 2.8135\tLR: 0.100000\n",
            "Training Epoch: 3 [7552/50000]\tLoss: 2.6952\tLR: 0.100000\n",
            "Training Epoch: 3 [7680/50000]\tLoss: 2.8925\tLR: 0.100000\n",
            "Training Epoch: 3 [7808/50000]\tLoss: 3.1066\tLR: 0.100000\n",
            "Training Epoch: 3 [7936/50000]\tLoss: 3.0621\tLR: 0.100000\n",
            "Training Epoch: 3 [8064/50000]\tLoss: 2.9580\tLR: 0.100000\n",
            "Training Epoch: 3 [8192/50000]\tLoss: 3.1004\tLR: 0.100000\n",
            "Training Epoch: 3 [8320/50000]\tLoss: 2.8511\tLR: 0.100000\n",
            "Training Epoch: 3 [8448/50000]\tLoss: 2.6876\tLR: 0.100000\n",
            "Training Epoch: 3 [8576/50000]\tLoss: 2.8564\tLR: 0.100000\n",
            "Training Epoch: 3 [8704/50000]\tLoss: 2.9188\tLR: 0.100000\n",
            "Training Epoch: 3 [8832/50000]\tLoss: 3.0884\tLR: 0.100000\n",
            "Training Epoch: 3 [8960/50000]\tLoss: 2.8719\tLR: 0.100000\n",
            "Training Epoch: 3 [9088/50000]\tLoss: 2.7370\tLR: 0.100000\n",
            "Training Epoch: 3 [9216/50000]\tLoss: 2.9140\tLR: 0.100000\n",
            "Training Epoch: 3 [9344/50000]\tLoss: 3.0752\tLR: 0.100000\n",
            "Training Epoch: 3 [9472/50000]\tLoss: 2.8992\tLR: 0.100000\n",
            "Training Epoch: 3 [9600/50000]\tLoss: 3.2469\tLR: 0.100000\n",
            "Training Epoch: 3 [9728/50000]\tLoss: 3.0635\tLR: 0.100000\n",
            "Training Epoch: 3 [9856/50000]\tLoss: 2.8265\tLR: 0.100000\n",
            "Training Epoch: 3 [9984/50000]\tLoss: 2.8822\tLR: 0.100000\n",
            "Training Epoch: 3 [10112/50000]\tLoss: 3.0630\tLR: 0.100000\n",
            "Training Epoch: 3 [10240/50000]\tLoss: 2.8714\tLR: 0.100000\n",
            "Training Epoch: 3 [10368/50000]\tLoss: 2.8866\tLR: 0.100000\n",
            "Training Epoch: 3 [10496/50000]\tLoss: 2.8776\tLR: 0.100000\n",
            "Training Epoch: 3 [10624/50000]\tLoss: 2.8717\tLR: 0.100000\n",
            "Training Epoch: 3 [10752/50000]\tLoss: 2.9213\tLR: 0.100000\n",
            "Training Epoch: 3 [10880/50000]\tLoss: 2.9490\tLR: 0.100000\n",
            "Training Epoch: 3 [11008/50000]\tLoss: 2.8550\tLR: 0.100000\n",
            "Training Epoch: 3 [11136/50000]\tLoss: 2.9337\tLR: 0.100000\n",
            "Training Epoch: 3 [11264/50000]\tLoss: 2.6465\tLR: 0.100000\n",
            "Training Epoch: 3 [11392/50000]\tLoss: 2.7474\tLR: 0.100000\n",
            "Training Epoch: 3 [11520/50000]\tLoss: 2.7889\tLR: 0.100000\n",
            "Training Epoch: 3 [11648/50000]\tLoss: 2.9120\tLR: 0.100000\n",
            "Training Epoch: 3 [11776/50000]\tLoss: 2.9110\tLR: 0.100000\n",
            "Training Epoch: 3 [11904/50000]\tLoss: 3.1063\tLR: 0.100000\n",
            "Training Epoch: 3 [12032/50000]\tLoss: 3.0013\tLR: 0.100000\n",
            "Training Epoch: 3 [12160/50000]\tLoss: 2.9291\tLR: 0.100000\n",
            "Training Epoch: 3 [12288/50000]\tLoss: 2.9975\tLR: 0.100000\n",
            "Training Epoch: 3 [12416/50000]\tLoss: 2.7423\tLR: 0.100000\n",
            "Training Epoch: 3 [12544/50000]\tLoss: 2.7382\tLR: 0.100000\n",
            "Training Epoch: 3 [12672/50000]\tLoss: 2.8947\tLR: 0.100000\n",
            "Training Epoch: 3 [12800/50000]\tLoss: 2.7301\tLR: 0.100000\n",
            "Training Epoch: 3 [12928/50000]\tLoss: 2.8073\tLR: 0.100000\n",
            "Training Epoch: 3 [13056/50000]\tLoss: 2.6209\tLR: 0.100000\n",
            "Training Epoch: 3 [13184/50000]\tLoss: 2.8870\tLR: 0.100000\n",
            "Training Epoch: 3 [13312/50000]\tLoss: 3.0188\tLR: 0.100000\n",
            "Training Epoch: 3 [13440/50000]\tLoss: 2.8475\tLR: 0.100000\n",
            "Training Epoch: 3 [13568/50000]\tLoss: 2.8800\tLR: 0.100000\n",
            "Training Epoch: 3 [13696/50000]\tLoss: 2.8699\tLR: 0.100000\n",
            "Training Epoch: 3 [13824/50000]\tLoss: 2.8066\tLR: 0.100000\n",
            "Training Epoch: 3 [13952/50000]\tLoss: 2.9133\tLR: 0.100000\n",
            "Training Epoch: 3 [14080/50000]\tLoss: 3.0331\tLR: 0.100000\n",
            "Training Epoch: 3 [14208/50000]\tLoss: 2.9387\tLR: 0.100000\n",
            "Training Epoch: 3 [14336/50000]\tLoss: 2.8053\tLR: 0.100000\n",
            "Training Epoch: 3 [14464/50000]\tLoss: 2.4354\tLR: 0.100000\n",
            "Training Epoch: 3 [14592/50000]\tLoss: 2.5569\tLR: 0.100000\n",
            "Training Epoch: 3 [14720/50000]\tLoss: 2.9152\tLR: 0.100000\n",
            "Training Epoch: 3 [14848/50000]\tLoss: 2.8229\tLR: 0.100000\n",
            "Training Epoch: 3 [14976/50000]\tLoss: 3.0457\tLR: 0.100000\n",
            "Training Epoch: 3 [15104/50000]\tLoss: 2.8577\tLR: 0.100000\n",
            "Training Epoch: 3 [15232/50000]\tLoss: 2.9585\tLR: 0.100000\n",
            "Training Epoch: 3 [15360/50000]\tLoss: 2.9087\tLR: 0.100000\n",
            "Training Epoch: 3 [15488/50000]\tLoss: 2.7866\tLR: 0.100000\n",
            "Training Epoch: 3 [15616/50000]\tLoss: 2.7368\tLR: 0.100000\n",
            "Training Epoch: 3 [15744/50000]\tLoss: 2.7924\tLR: 0.100000\n",
            "Training Epoch: 3 [15872/50000]\tLoss: 2.8804\tLR: 0.100000\n",
            "Training Epoch: 3 [16000/50000]\tLoss: 2.5536\tLR: 0.100000\n",
            "Training Epoch: 3 [16128/50000]\tLoss: 3.0770\tLR: 0.100000\n",
            "Training Epoch: 3 [16256/50000]\tLoss: 2.8071\tLR: 0.100000\n",
            "Training Epoch: 3 [16384/50000]\tLoss: 2.5708\tLR: 0.100000\n",
            "Training Epoch: 3 [16512/50000]\tLoss: 2.5037\tLR: 0.100000\n",
            "Training Epoch: 3 [16640/50000]\tLoss: 2.9501\tLR: 0.100000\n",
            "Training Epoch: 3 [16768/50000]\tLoss: 2.9925\tLR: 0.100000\n",
            "Training Epoch: 3 [16896/50000]\tLoss: 2.8852\tLR: 0.100000\n",
            "Training Epoch: 3 [17024/50000]\tLoss: 2.7768\tLR: 0.100000\n",
            "Training Epoch: 3 [17152/50000]\tLoss: 3.0679\tLR: 0.100000\n",
            "Training Epoch: 3 [17280/50000]\tLoss: 2.9779\tLR: 0.100000\n",
            "Training Epoch: 3 [17408/50000]\tLoss: 2.5599\tLR: 0.100000\n",
            "Training Epoch: 3 [17536/50000]\tLoss: 2.7455\tLR: 0.100000\n",
            "Training Epoch: 3 [17664/50000]\tLoss: 2.8338\tLR: 0.100000\n",
            "Training Epoch: 3 [17792/50000]\tLoss: 2.8393\tLR: 0.100000\n",
            "Training Epoch: 3 [17920/50000]\tLoss: 2.9076\tLR: 0.100000\n",
            "Training Epoch: 3 [18048/50000]\tLoss: 3.1850\tLR: 0.100000\n",
            "Training Epoch: 3 [18176/50000]\tLoss: 2.9222\tLR: 0.100000\n",
            "Training Epoch: 3 [18304/50000]\tLoss: 2.7619\tLR: 0.100000\n",
            "Training Epoch: 3 [18432/50000]\tLoss: 2.7457\tLR: 0.100000\n",
            "Training Epoch: 3 [18560/50000]\tLoss: 2.9469\tLR: 0.100000\n",
            "Training Epoch: 3 [18688/50000]\tLoss: 2.7591\tLR: 0.100000\n",
            "Training Epoch: 3 [18816/50000]\tLoss: 2.7357\tLR: 0.100000\n",
            "Training Epoch: 3 [18944/50000]\tLoss: 2.7161\tLR: 0.100000\n",
            "Training Epoch: 3 [19072/50000]\tLoss: 2.9279\tLR: 0.100000\n",
            "Training Epoch: 3 [19200/50000]\tLoss: 2.9301\tLR: 0.100000\n",
            "Training Epoch: 3 [19328/50000]\tLoss: 2.7337\tLR: 0.100000\n",
            "Training Epoch: 3 [19456/50000]\tLoss: 2.9110\tLR: 0.100000\n",
            "Training Epoch: 3 [19584/50000]\tLoss: 2.7061\tLR: 0.100000\n",
            "Training Epoch: 3 [19712/50000]\tLoss: 2.8533\tLR: 0.100000\n",
            "Training Epoch: 3 [19840/50000]\tLoss: 2.8788\tLR: 0.100000\n",
            "Training Epoch: 3 [19968/50000]\tLoss: 2.7634\tLR: 0.100000\n",
            "Training Epoch: 3 [20096/50000]\tLoss: 2.8368\tLR: 0.100000\n",
            "Training Epoch: 3 [20224/50000]\tLoss: 2.7448\tLR: 0.100000\n",
            "Training Epoch: 3 [20352/50000]\tLoss: 2.8452\tLR: 0.100000\n",
            "Training Epoch: 3 [20480/50000]\tLoss: 2.4839\tLR: 0.100000\n",
            "Training Epoch: 3 [20608/50000]\tLoss: 2.8427\tLR: 0.100000\n",
            "Training Epoch: 3 [20736/50000]\tLoss: 2.7150\tLR: 0.100000\n",
            "Training Epoch: 3 [20864/50000]\tLoss: 3.1430\tLR: 0.100000\n",
            "Training Epoch: 3 [20992/50000]\tLoss: 2.7328\tLR: 0.100000\n",
            "Training Epoch: 3 [21120/50000]\tLoss: 2.6676\tLR: 0.100000\n",
            "Training Epoch: 3 [21248/50000]\tLoss: 2.6630\tLR: 0.100000\n",
            "Training Epoch: 3 [21376/50000]\tLoss: 2.6797\tLR: 0.100000\n",
            "Training Epoch: 3 [21504/50000]\tLoss: 2.7465\tLR: 0.100000\n",
            "Training Epoch: 3 [21632/50000]\tLoss: 2.6585\tLR: 0.100000\n",
            "Training Epoch: 3 [21760/50000]\tLoss: 2.6052\tLR: 0.100000\n",
            "Training Epoch: 3 [21888/50000]\tLoss: 2.6171\tLR: 0.100000\n",
            "Training Epoch: 3 [22016/50000]\tLoss: 2.5570\tLR: 0.100000\n",
            "Training Epoch: 3 [22144/50000]\tLoss: 2.6138\tLR: 0.100000\n",
            "Training Epoch: 3 [22272/50000]\tLoss: 2.7507\tLR: 0.100000\n",
            "Training Epoch: 3 [22400/50000]\tLoss: 2.8561\tLR: 0.100000\n",
            "Training Epoch: 3 [22528/50000]\tLoss: 2.9410\tLR: 0.100000\n",
            "Training Epoch: 3 [22656/50000]\tLoss: 2.5690\tLR: 0.100000\n",
            "Training Epoch: 3 [22784/50000]\tLoss: 2.7591\tLR: 0.100000\n",
            "Training Epoch: 3 [22912/50000]\tLoss: 2.8648\tLR: 0.100000\n",
            "Training Epoch: 3 [23040/50000]\tLoss: 2.8494\tLR: 0.100000\n",
            "Training Epoch: 3 [23168/50000]\tLoss: 2.6628\tLR: 0.100000\n",
            "Training Epoch: 3 [23296/50000]\tLoss: 2.6903\tLR: 0.100000\n",
            "Training Epoch: 3 [23424/50000]\tLoss: 2.6278\tLR: 0.100000\n",
            "Training Epoch: 3 [23552/50000]\tLoss: 2.5310\tLR: 0.100000\n",
            "Training Epoch: 3 [23680/50000]\tLoss: 2.7016\tLR: 0.100000\n",
            "Training Epoch: 3 [23808/50000]\tLoss: 2.7815\tLR: 0.100000\n",
            "Training Epoch: 3 [23936/50000]\tLoss: 3.0176\tLR: 0.100000\n",
            "Training Epoch: 3 [24064/50000]\tLoss: 2.8937\tLR: 0.100000\n",
            "Training Epoch: 3 [24192/50000]\tLoss: 2.7232\tLR: 0.100000\n",
            "Training Epoch: 3 [24320/50000]\tLoss: 2.8860\tLR: 0.100000\n",
            "Training Epoch: 3 [24448/50000]\tLoss: 2.5515\tLR: 0.100000\n",
            "Training Epoch: 3 [24576/50000]\tLoss: 2.6985\tLR: 0.100000\n",
            "Training Epoch: 3 [24704/50000]\tLoss: 2.6995\tLR: 0.100000\n",
            "Training Epoch: 3 [24832/50000]\tLoss: 2.7620\tLR: 0.100000\n",
            "Training Epoch: 3 [24960/50000]\tLoss: 2.9039\tLR: 0.100000\n",
            "Training Epoch: 3 [25088/50000]\tLoss: 2.8174\tLR: 0.100000\n",
            "Training Epoch: 3 [25216/50000]\tLoss: 2.5173\tLR: 0.100000\n",
            "Training Epoch: 3 [25344/50000]\tLoss: 2.9807\tLR: 0.100000\n",
            "Training Epoch: 3 [25472/50000]\tLoss: 2.9766\tLR: 0.100000\n",
            "Training Epoch: 3 [25600/50000]\tLoss: 2.7210\tLR: 0.100000\n",
            "Training Epoch: 3 [25728/50000]\tLoss: 2.5737\tLR: 0.100000\n",
            "Training Epoch: 3 [25856/50000]\tLoss: 2.6650\tLR: 0.100000\n",
            "Training Epoch: 3 [25984/50000]\tLoss: 2.8370\tLR: 0.100000\n",
            "Training Epoch: 3 [26112/50000]\tLoss: 2.9849\tLR: 0.100000\n",
            "Training Epoch: 3 [26240/50000]\tLoss: 2.8171\tLR: 0.100000\n",
            "Training Epoch: 3 [26368/50000]\tLoss: 2.8853\tLR: 0.100000\n",
            "Training Epoch: 3 [26496/50000]\tLoss: 2.9085\tLR: 0.100000\n",
            "Training Epoch: 3 [26624/50000]\tLoss: 2.9117\tLR: 0.100000\n",
            "Training Epoch: 3 [26752/50000]\tLoss: 2.8461\tLR: 0.100000\n",
            "Training Epoch: 3 [26880/50000]\tLoss: 2.5770\tLR: 0.100000\n",
            "Training Epoch: 3 [27008/50000]\tLoss: 3.0122\tLR: 0.100000\n",
            "Training Epoch: 3 [27136/50000]\tLoss: 2.7236\tLR: 0.100000\n",
            "Training Epoch: 3 [27264/50000]\tLoss: 2.8674\tLR: 0.100000\n",
            "Training Epoch: 3 [27392/50000]\tLoss: 2.5068\tLR: 0.100000\n",
            "Training Epoch: 3 [27520/50000]\tLoss: 2.7490\tLR: 0.100000\n",
            "Training Epoch: 3 [27648/50000]\tLoss: 2.6983\tLR: 0.100000\n",
            "Training Epoch: 3 [27776/50000]\tLoss: 2.7388\tLR: 0.100000\n",
            "Training Epoch: 3 [27904/50000]\tLoss: 2.7304\tLR: 0.100000\n",
            "Training Epoch: 3 [28032/50000]\tLoss: 2.8419\tLR: 0.100000\n",
            "Training Epoch: 3 [28160/50000]\tLoss: 2.6701\tLR: 0.100000\n",
            "Training Epoch: 3 [28288/50000]\tLoss: 2.5689\tLR: 0.100000\n",
            "Training Epoch: 3 [28416/50000]\tLoss: 2.8197\tLR: 0.100000\n",
            "Training Epoch: 3 [28544/50000]\tLoss: 2.5820\tLR: 0.100000\n",
            "Training Epoch: 3 [28672/50000]\tLoss: 2.7051\tLR: 0.100000\n",
            "Training Epoch: 3 [28800/50000]\tLoss: 2.4383\tLR: 0.100000\n",
            "Training Epoch: 3 [28928/50000]\tLoss: 2.6763\tLR: 0.100000\n",
            "Training Epoch: 3 [29056/50000]\tLoss: 2.7947\tLR: 0.100000\n",
            "Training Epoch: 3 [29184/50000]\tLoss: 2.6477\tLR: 0.100000\n",
            "Training Epoch: 3 [29312/50000]\tLoss: 2.7446\tLR: 0.100000\n",
            "Training Epoch: 3 [29440/50000]\tLoss: 2.8873\tLR: 0.100000\n",
            "Training Epoch: 3 [29568/50000]\tLoss: 2.6948\tLR: 0.100000\n",
            "Training Epoch: 3 [29696/50000]\tLoss: 2.4739\tLR: 0.100000\n",
            "Training Epoch: 3 [29824/50000]\tLoss: 2.6967\tLR: 0.100000\n",
            "Training Epoch: 3 [29952/50000]\tLoss: 2.7394\tLR: 0.100000\n",
            "Training Epoch: 3 [30080/50000]\tLoss: 2.7038\tLR: 0.100000\n",
            "Training Epoch: 3 [30208/50000]\tLoss: 2.7894\tLR: 0.100000\n",
            "Training Epoch: 3 [30336/50000]\tLoss: 2.6379\tLR: 0.100000\n",
            "Training Epoch: 3 [30464/50000]\tLoss: 2.6000\tLR: 0.100000\n",
            "Training Epoch: 3 [30592/50000]\tLoss: 2.6193\tLR: 0.100000\n",
            "Training Epoch: 3 [30720/50000]\tLoss: 2.6684\tLR: 0.100000\n",
            "Training Epoch: 3 [30848/50000]\tLoss: 2.6192\tLR: 0.100000\n",
            "Training Epoch: 3 [30976/50000]\tLoss: 2.6667\tLR: 0.100000\n",
            "Training Epoch: 3 [31104/50000]\tLoss: 2.7829\tLR: 0.100000\n",
            "Training Epoch: 3 [31232/50000]\tLoss: 2.5710\tLR: 0.100000\n",
            "Training Epoch: 3 [31360/50000]\tLoss: 2.6081\tLR: 0.100000\n",
            "Training Epoch: 3 [31488/50000]\tLoss: 2.8099\tLR: 0.100000\n",
            "Training Epoch: 3 [31616/50000]\tLoss: 2.4946\tLR: 0.100000\n",
            "Training Epoch: 3 [31744/50000]\tLoss: 2.4861\tLR: 0.100000\n",
            "Training Epoch: 3 [31872/50000]\tLoss: 2.4847\tLR: 0.100000\n",
            "Training Epoch: 3 [32000/50000]\tLoss: 2.6654\tLR: 0.100000\n",
            "Training Epoch: 3 [32128/50000]\tLoss: 2.4942\tLR: 0.100000\n",
            "Training Epoch: 3 [32256/50000]\tLoss: 2.5942\tLR: 0.100000\n",
            "Training Epoch: 3 [32384/50000]\tLoss: 2.9526\tLR: 0.100000\n",
            "Training Epoch: 3 [32512/50000]\tLoss: 2.6343\tLR: 0.100000\n",
            "Training Epoch: 3 [32640/50000]\tLoss: 2.6935\tLR: 0.100000\n",
            "Training Epoch: 3 [32768/50000]\tLoss: 2.6055\tLR: 0.100000\n",
            "Training Epoch: 3 [32896/50000]\tLoss: 2.7106\tLR: 0.100000\n",
            "Training Epoch: 3 [33024/50000]\tLoss: 2.5510\tLR: 0.100000\n",
            "Training Epoch: 3 [33152/50000]\tLoss: 2.6410\tLR: 0.100000\n",
            "Training Epoch: 3 [33280/50000]\tLoss: 2.5687\tLR: 0.100000\n",
            "Training Epoch: 3 [33408/50000]\tLoss: 2.6036\tLR: 0.100000\n",
            "Training Epoch: 3 [33536/50000]\tLoss: 2.8775\tLR: 0.100000\n",
            "Training Epoch: 3 [33664/50000]\tLoss: 2.6975\tLR: 0.100000\n",
            "Training Epoch: 3 [33792/50000]\tLoss: 2.5790\tLR: 0.100000\n",
            "Training Epoch: 3 [33920/50000]\tLoss: 2.9459\tLR: 0.100000\n",
            "Training Epoch: 3 [34048/50000]\tLoss: 2.5577\tLR: 0.100000\n",
            "Training Epoch: 3 [34176/50000]\tLoss: 2.9651\tLR: 0.100000\n",
            "Training Epoch: 3 [34304/50000]\tLoss: 2.6358\tLR: 0.100000\n",
            "Training Epoch: 3 [34432/50000]\tLoss: 2.6180\tLR: 0.100000\n",
            "Training Epoch: 3 [34560/50000]\tLoss: 2.8434\tLR: 0.100000\n",
            "Training Epoch: 3 [34688/50000]\tLoss: 2.9089\tLR: 0.100000\n",
            "Training Epoch: 3 [34816/50000]\tLoss: 2.7268\tLR: 0.100000\n",
            "Training Epoch: 3 [34944/50000]\tLoss: 2.8482\tLR: 0.100000\n",
            "Training Epoch: 3 [35072/50000]\tLoss: 2.9286\tLR: 0.100000\n",
            "Training Epoch: 3 [35200/50000]\tLoss: 2.7136\tLR: 0.100000\n",
            "Training Epoch: 3 [35328/50000]\tLoss: 2.4172\tLR: 0.100000\n",
            "Training Epoch: 3 [35456/50000]\tLoss: 2.4408\tLR: 0.100000\n",
            "Training Epoch: 3 [35584/50000]\tLoss: 2.9342\tLR: 0.100000\n",
            "Training Epoch: 3 [35712/50000]\tLoss: 2.5818\tLR: 0.100000\n",
            "Training Epoch: 3 [35840/50000]\tLoss: 2.8432\tLR: 0.100000\n",
            "Training Epoch: 3 [35968/50000]\tLoss: 2.6808\tLR: 0.100000\n",
            "Training Epoch: 3 [36096/50000]\tLoss: 2.7222\tLR: 0.100000\n",
            "Training Epoch: 3 [36224/50000]\tLoss: 2.9080\tLR: 0.100000\n",
            "Training Epoch: 3 [36352/50000]\tLoss: 2.8529\tLR: 0.100000\n",
            "Training Epoch: 3 [36480/50000]\tLoss: 2.7070\tLR: 0.100000\n",
            "Training Epoch: 3 [36608/50000]\tLoss: 2.7259\tLR: 0.100000\n",
            "Training Epoch: 3 [36736/50000]\tLoss: 2.7180\tLR: 0.100000\n",
            "Training Epoch: 3 [36864/50000]\tLoss: 2.5909\tLR: 0.100000\n",
            "Training Epoch: 3 [36992/50000]\tLoss: 2.8406\tLR: 0.100000\n",
            "Training Epoch: 3 [37120/50000]\tLoss: 2.6523\tLR: 0.100000\n",
            "Training Epoch: 3 [37248/50000]\tLoss: 2.5741\tLR: 0.100000\n",
            "Training Epoch: 3 [37376/50000]\tLoss: 2.5965\tLR: 0.100000\n",
            "Training Epoch: 3 [37504/50000]\tLoss: 2.7288\tLR: 0.100000\n",
            "Training Epoch: 3 [37632/50000]\tLoss: 2.7950\tLR: 0.100000\n",
            "Training Epoch: 3 [37760/50000]\tLoss: 2.4283\tLR: 0.100000\n",
            "Training Epoch: 3 [37888/50000]\tLoss: 2.9075\tLR: 0.100000\n",
            "Training Epoch: 3 [38016/50000]\tLoss: 2.6748\tLR: 0.100000\n",
            "Training Epoch: 3 [38144/50000]\tLoss: 2.5634\tLR: 0.100000\n",
            "Training Epoch: 3 [44160/50000]\tLoss: 2.5715\tLR: 0.100000\n",
            "Training Epoch: 3 [44288/50000]\tLoss: 2.6085\tLR: 0.100000\n",
            "Training Epoch: 3 [44416/50000]\tLoss: 2.7591\tLR: 0.100000\n",
            "Training Epoch: 3 [44544/50000]\tLoss: 2.7809\tLR: 0.100000\n",
            "Training Epoch: 3 [44672/50000]\tLoss: 2.7616\tLR: 0.100000\n",
            "Training Epoch: 3 [44800/50000]\tLoss: 2.5474\tLR: 0.100000\n",
            "Training Epoch: 3 [44928/50000]\tLoss: 2.5153\tLR: 0.100000\n",
            "Training Epoch: 3 [45056/50000]\tLoss: 2.5023\tLR: 0.100000\n",
            "Training Epoch: 3 [45184/50000]\tLoss: 2.4160\tLR: 0.100000\n",
            "Training Epoch: 3 [45312/50000]\tLoss: 2.5964\tLR: 0.100000\n",
            "Training Epoch: 3 [45440/50000]\tLoss: 2.5452\tLR: 0.100000\n",
            "Training Epoch: 3 [45568/50000]\tLoss: 2.5971\tLR: 0.100000\n",
            "Training Epoch: 3 [45696/50000]\tLoss: 2.6505\tLR: 0.100000\n",
            "Training Epoch: 3 [45824/50000]\tLoss: 2.6656\tLR: 0.100000\n",
            "Training Epoch: 3 [45952/50000]\tLoss: 2.6780\tLR: 0.100000\n",
            "Training Epoch: 3 [46080/50000]\tLoss: 2.7118\tLR: 0.100000\n",
            "Training Epoch: 3 [46208/50000]\tLoss: 2.3855\tLR: 0.100000\n",
            "Training Epoch: 3 [46336/50000]\tLoss: 2.4256\tLR: 0.100000\n",
            "Training Epoch: 3 [46464/50000]\tLoss: 2.6160\tLR: 0.100000\n",
            "Training Epoch: 3 [46592/50000]\tLoss: 2.7324\tLR: 0.100000\n",
            "Training Epoch: 3 [46720/50000]\tLoss: 2.6869\tLR: 0.100000\n",
            "Training Epoch: 3 [46848/50000]\tLoss: 2.6128\tLR: 0.100000\n",
            "Training Epoch: 3 [46976/50000]\tLoss: 2.7156\tLR: 0.100000\n",
            "Training Epoch: 3 [47104/50000]\tLoss: 2.5766\tLR: 0.100000\n",
            "Training Epoch: 3 [47232/50000]\tLoss: 2.5137\tLR: 0.100000\n",
            "Training Epoch: 3 [47360/50000]\tLoss: 2.7232\tLR: 0.100000\n",
            "Training Epoch: 3 [47488/50000]\tLoss: 2.7702\tLR: 0.100000\n",
            "Training Epoch: 3 [47616/50000]\tLoss: 2.8183\tLR: 0.100000\n",
            "Training Epoch: 3 [47744/50000]\tLoss: 2.5854\tLR: 0.100000\n",
            "Training Epoch: 3 [47872/50000]\tLoss: 2.9790\tLR: 0.100000\n",
            "Training Epoch: 3 [48000/50000]\tLoss: 2.5781\tLR: 0.100000\n",
            "Training Epoch: 3 [48128/50000]\tLoss: 2.6269\tLR: 0.100000\n",
            "Training Epoch: 3 [48256/50000]\tLoss: 2.8068\tLR: 0.100000\n",
            "Training Epoch: 3 [48384/50000]\tLoss: 2.6141\tLR: 0.100000\n",
            "Training Epoch: 3 [48512/50000]\tLoss: 3.0202\tLR: 0.100000\n",
            "Training Epoch: 3 [48640/50000]\tLoss: 2.8268\tLR: 0.100000\n",
            "Training Epoch: 3 [48768/50000]\tLoss: 2.7413\tLR: 0.100000\n",
            "Training Epoch: 3 [48896/50000]\tLoss: 2.6668\tLR: 0.100000\n",
            "Training Epoch: 3 [49024/50000]\tLoss: 2.5979\tLR: 0.100000\n",
            "Training Epoch: 3 [49152/50000]\tLoss: 2.6190\tLR: 0.100000\n",
            "Training Epoch: 3 [49280/50000]\tLoss: 2.4165\tLR: 0.100000\n",
            "Training Epoch: 3 [49408/50000]\tLoss: 2.6636\tLR: 0.100000\n",
            "Training Epoch: 3 [49536/50000]\tLoss: 2.7595\tLR: 0.100000\n",
            "Training Epoch: 3 [49664/50000]\tLoss: 2.7786\tLR: 0.100000\n",
            "Training Epoch: 3 [49792/50000]\tLoss: 2.7499\tLR: 0.100000\n",
            "Training Epoch: 3 [49920/50000]\tLoss: 2.6803\tLR: 0.100000\n",
            "Training Epoch: 3 [50000/50000]\tLoss: 2.6651\tLR: 0.100000\n",
            "Test set: Average loss: 0.0206, Accuracy: 0.3199\n",
            "\n",
            "Training Epoch: 4 [128/50000]\tLoss: 2.5029\tLR: 0.100000\n",
            "Training Epoch: 4 [256/50000]\tLoss: 2.4466\tLR: 0.100000\n",
            "Training Epoch: 4 [384/50000]\tLoss: 2.4289\tLR: 0.100000\n",
            "Training Epoch: 4 [512/50000]\tLoss: 2.6343\tLR: 0.100000\n",
            "Training Epoch: 4 [640/50000]\tLoss: 2.6571\tLR: 0.100000\n",
            "Training Epoch: 4 [768/50000]\tLoss: 2.5798\tLR: 0.100000\n",
            "Training Epoch: 4 [896/50000]\tLoss: 2.5822\tLR: 0.100000\n",
            "Training Epoch: 4 [1024/50000]\tLoss: 2.3818\tLR: 0.100000\n",
            "Training Epoch: 4 [1152/50000]\tLoss: 2.3544\tLR: 0.100000\n",
            "Training Epoch: 4 [1280/50000]\tLoss: 2.5525\tLR: 0.100000\n",
            "Training Epoch: 4 [1408/50000]\tLoss: 2.7181\tLR: 0.100000\n",
            "Training Epoch: 4 [1536/50000]\tLoss: 2.4034\tLR: 0.100000\n",
            "Training Epoch: 4 [1664/50000]\tLoss: 2.7744\tLR: 0.100000\n",
            "Training Epoch: 4 [1792/50000]\tLoss: 2.5217\tLR: 0.100000\n",
            "Training Epoch: 4 [1920/50000]\tLoss: 2.3679\tLR: 0.100000\n",
            "Training Epoch: 4 [2048/50000]\tLoss: 2.7399\tLR: 0.100000\n",
            "Training Epoch: 4 [2176/50000]\tLoss: 2.4189\tLR: 0.100000\n",
            "Training Epoch: 4 [2304/50000]\tLoss: 2.5166\tLR: 0.100000\n",
            "Training Epoch: 4 [2432/50000]\tLoss: 2.5759\tLR: 0.100000\n",
            "Training Epoch: 4 [2560/50000]\tLoss: 2.3059\tLR: 0.100000\n",
            "Training Epoch: 4 [2688/50000]\tLoss: 2.6288\tLR: 0.100000\n",
            "Training Epoch: 4 [2816/50000]\tLoss: 2.6630\tLR: 0.100000\n",
            "Training Epoch: 4 [2944/50000]\tLoss: 2.7757\tLR: 0.100000\n",
            "Training Epoch: 4 [3072/50000]\tLoss: 2.6257\tLR: 0.100000\n",
            "Training Epoch: 4 [3200/50000]\tLoss: 2.3901\tLR: 0.100000\n",
            "Training Epoch: 4 [3328/50000]\tLoss: 2.4332\tLR: 0.100000\n",
            "Training Epoch: 4 [3456/50000]\tLoss: 2.4132\tLR: 0.100000\n",
            "Training Epoch: 4 [3584/50000]\tLoss: 2.2647\tLR: 0.100000\n",
            "Training Epoch: 4 [3712/50000]\tLoss: 2.4459\tLR: 0.100000\n",
            "Training Epoch: 4 [3840/50000]\tLoss: 2.5744\tLR: 0.100000\n",
            "Training Epoch: 4 [3968/50000]\tLoss: 2.1783\tLR: 0.100000\n",
            "Training Epoch: 4 [4096/50000]\tLoss: 2.5795\tLR: 0.100000\n",
            "Training Epoch: 4 [4224/50000]\tLoss: 2.5103\tLR: 0.100000\n",
            "Training Epoch: 4 [4352/50000]\tLoss: 2.8747\tLR: 0.100000\n",
            "Training Epoch: 4 [4480/50000]\tLoss: 2.6560\tLR: 0.100000\n",
            "Training Epoch: 4 [4608/50000]\tLoss: 2.5431\tLR: 0.100000\n",
            "Training Epoch: 4 [4736/50000]\tLoss: 2.5254\tLR: 0.100000\n",
            "Training Epoch: 4 [4864/50000]\tLoss: 2.7376\tLR: 0.100000\n",
            "Training Epoch: 4 [4992/50000]\tLoss: 2.3863\tLR: 0.100000\n",
            "Training Epoch: 4 [5120/50000]\tLoss: 2.4100\tLR: 0.100000\n",
            "Training Epoch: 4 [5248/50000]\tLoss: 2.3237\tLR: 0.100000\n",
            "Training Epoch: 4 [5376/50000]\tLoss: 2.5569\tLR: 0.100000\n",
            "Training Epoch: 4 [5504/50000]\tLoss: 2.3609\tLR: 0.100000\n",
            "Training Epoch: 4 [5632/50000]\tLoss: 2.4377\tLR: 0.100000\n",
            "Training Epoch: 4 [5760/50000]\tLoss: 2.4673\tLR: 0.100000\n",
            "Training Epoch: 4 [5888/50000]\tLoss: 2.3435\tLR: 0.100000\n",
            "Training Epoch: 4 [6016/50000]\tLoss: 2.4730\tLR: 0.100000\n",
            "Training Epoch: 4 [6144/50000]\tLoss: 2.4535\tLR: 0.100000\n",
            "Training Epoch: 4 [6272/50000]\tLoss: 2.4409\tLR: 0.100000\n",
            "Training Epoch: 4 [6400/50000]\tLoss: 2.5808\tLR: 0.100000\n",
            "Training Epoch: 4 [6528/50000]\tLoss: 2.6036\tLR: 0.100000\n",
            "Training Epoch: 4 [6656/50000]\tLoss: 2.5001\tLR: 0.100000\n",
            "Training Epoch: 4 [6784/50000]\tLoss: 2.5798\tLR: 0.100000\n",
            "Training Epoch: 4 [6912/50000]\tLoss: 2.7454\tLR: 0.100000\n",
            "Training Epoch: 4 [7040/50000]\tLoss: 2.5015\tLR: 0.100000\n",
            "Training Epoch: 4 [7168/50000]\tLoss: 2.7410\tLR: 0.100000\n",
            "Training Epoch: 4 [7296/50000]\tLoss: 2.4359\tLR: 0.100000\n",
            "Training Epoch: 4 [7424/50000]\tLoss: 2.5715\tLR: 0.100000\n",
            "Training Epoch: 4 [7552/50000]\tLoss: 2.2039\tLR: 0.100000\n",
            "Training Epoch: 4 [7680/50000]\tLoss: 2.3665\tLR: 0.100000\n",
            "Training Epoch: 4 [7808/50000]\tLoss: 2.4380\tLR: 0.100000\n",
            "Training Epoch: 4 [7936/50000]\tLoss: 2.5139\tLR: 0.100000\n",
            "Training Epoch: 4 [8064/50000]\tLoss: 2.4127\tLR: 0.100000\n",
            "Training Epoch: 4 [8192/50000]\tLoss: 2.3444\tLR: 0.100000\n",
            "Training Epoch: 4 [8320/50000]\tLoss: 2.3565\tLR: 0.100000\n",
            "Training Epoch: 4 [8448/50000]\tLoss: 2.5933\tLR: 0.100000\n",
            "Training Epoch: 4 [8576/50000]\tLoss: 2.5069\tLR: 0.100000\n",
            "Training Epoch: 4 [8704/50000]\tLoss: 2.4767\tLR: 0.100000\n",
            "Training Epoch: 4 [8832/50000]\tLoss: 2.4739\tLR: 0.100000\n",
            "Training Epoch: 4 [8960/50000]\tLoss: 2.5798\tLR: 0.100000\n",
            "Training Epoch: 4 [9088/50000]\tLoss: 2.4117\tLR: 0.100000\n",
            "Training Epoch: 4 [9216/50000]\tLoss: 2.3993\tLR: 0.100000\n",
            "Training Epoch: 4 [9344/50000]\tLoss: 2.5476\tLR: 0.100000\n",
            "Training Epoch: 4 [9472/50000]\tLoss: 2.5034\tLR: 0.100000\n",
            "Training Epoch: 4 [9600/50000]\tLoss: 2.4027\tLR: 0.100000\n",
            "Training Epoch: 4 [9728/50000]\tLoss: 2.5752\tLR: 0.100000\n",
            "Training Epoch: 4 [9856/50000]\tLoss: 2.5361\tLR: 0.100000\n",
            "Training Epoch: 4 [9984/50000]\tLoss: 2.7666\tLR: 0.100000\n",
            "Training Epoch: 4 [10112/50000]\tLoss: 2.4886\tLR: 0.100000\n",
            "Training Epoch: 4 [10240/50000]\tLoss: 3.0323\tLR: 0.100000\n",
            "Training Epoch: 4 [10368/50000]\tLoss: 2.5734\tLR: 0.100000\n",
            "Training Epoch: 4 [10496/50000]\tLoss: 2.0645\tLR: 0.100000\n",
            "Training Epoch: 4 [10624/50000]\tLoss: 2.3758\tLR: 0.100000\n",
            "Training Epoch: 4 [10752/50000]\tLoss: 2.4023\tLR: 0.100000\n",
            "Training Epoch: 4 [10880/50000]\tLoss: 2.4111\tLR: 0.100000\n",
            "Training Epoch: 4 [11008/50000]\tLoss: 2.3058\tLR: 0.100000\n",
            "Training Epoch: 4 [11136/50000]\tLoss: 2.2248\tLR: 0.100000\n",
            "Training Epoch: 4 [11264/50000]\tLoss: 2.4387\tLR: 0.100000\n",
            "Training Epoch: 4 [11392/50000]\tLoss: 2.7199\tLR: 0.100000\n",
            "Training Epoch: 4 [11520/50000]\tLoss: 2.2704\tLR: 0.100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYFLhac1h5p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}